{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682440739511,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "-0S_9bLWZ_g4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data  = pd.read_csv(\"framingham.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3055,
     "status": "ok",
     "timestamp": 1682440742563,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "dDcjpkhEZ_dR"
   },
   "outputs": [],
   "source": [
    "# for mathematical calculations\n",
    "import numpy as np\n",
    "\n",
    "# to visualize the data and results\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# to build the model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split , cross_val_score, RandomizedSearchCV\n",
    "\n",
    "\n",
    "# to get rid of the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1682440742564,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "JR5lsbNrZ_WW",
    "outputId": "6ec52489-5c2f-4095-d9b8-7f2d3c8587fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1682440742564,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "dAbW7JKAaGX9",
    "outputId": "7225e10e-bb9c-4d99-943b-ea045da8ba79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4240, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1682440742565,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "5aSwmqXnaL_c",
    "outputId": "ff451226-536e-4181-d86f-cd592ce62b6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4240 entries, 0 to 4239\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   male             4240 non-null   int64  \n",
      " 1   age              4240 non-null   int64  \n",
      " 2   education        4135 non-null   float64\n",
      " 3   currentSmoker    4240 non-null   int64  \n",
      " 4   cigsPerDay       4211 non-null   float64\n",
      " 5   BPMeds           4187 non-null   float64\n",
      " 6   prevalentStroke  4240 non-null   int64  \n",
      " 7   prevalentHyp     4240 non-null   int64  \n",
      " 8   diabetes         4240 non-null   int64  \n",
      " 9   totChol          4190 non-null   float64\n",
      " 10  sysBP            4240 non-null   float64\n",
      " 11  diaBP            4240 non-null   float64\n",
      " 12  BMI              4221 non-null   float64\n",
      " 13  heartRate        4239 non-null   float64\n",
      " 14  glucose          3852 non-null   float64\n",
      " 15  TenYearCHD       4240 non-null   int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 530.1 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1682440742566,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "ldQyJqmdaTuC",
    "outputId": "f1368973-fa36-40af-c5bc-9a58be0d113b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n",
      "       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n",
      "       'diaBP', 'BMI', 'heartRate', 'glucose', 'TenYearCHD'],\n",
      "      dtype='object')\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(data.select_dtypes(include=['float64','int64']).columns) #Printing the Names on all the Numerical Columns\n",
    "print(len(data.select_dtypes(include=['float64','int64']).columns)) #Printing the total number of Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1682440742566,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "THzYGk6naTp-",
    "outputId": "3d832d25-b2f6-4209-f957-fe33b0044157"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4135.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4211.000000</td>\n",
       "      <td>4187.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4190.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "      <td>4221.000000</td>\n",
       "      <td>4239.000000</td>\n",
       "      <td>3852.000000</td>\n",
       "      <td>4240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429245</td>\n",
       "      <td>49.580189</td>\n",
       "      <td>1.979444</td>\n",
       "      <td>0.494104</td>\n",
       "      <td>9.005937</td>\n",
       "      <td>0.029615</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.310613</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>236.699523</td>\n",
       "      <td>132.354599</td>\n",
       "      <td>82.897759</td>\n",
       "      <td>25.800801</td>\n",
       "      <td>75.878981</td>\n",
       "      <td>81.963655</td>\n",
       "      <td>0.151887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.495027</td>\n",
       "      <td>8.572942</td>\n",
       "      <td>1.019791</td>\n",
       "      <td>0.500024</td>\n",
       "      <td>11.922462</td>\n",
       "      <td>0.169544</td>\n",
       "      <td>0.076569</td>\n",
       "      <td>0.462799</td>\n",
       "      <td>0.158280</td>\n",
       "      <td>44.591284</td>\n",
       "      <td>22.033300</td>\n",
       "      <td>11.910394</td>\n",
       "      <td>4.079840</td>\n",
       "      <td>12.025348</td>\n",
       "      <td>23.954335</td>\n",
       "      <td>0.358953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.540000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>23.070000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>234.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>263.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>28.040000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>696.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>56.800000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              male          age    education  currentSmoker   cigsPerDay  \\\n",
       "count  4240.000000  4240.000000  4135.000000    4240.000000  4211.000000   \n",
       "mean      0.429245    49.580189     1.979444       0.494104     9.005937   \n",
       "std       0.495027     8.572942     1.019791       0.500024    11.922462   \n",
       "min       0.000000    32.000000     1.000000       0.000000     0.000000   \n",
       "25%       0.000000    42.000000     1.000000       0.000000     0.000000   \n",
       "50%       0.000000    49.000000     2.000000       0.000000     0.000000   \n",
       "75%       1.000000    56.000000     3.000000       1.000000    20.000000   \n",
       "max       1.000000    70.000000     4.000000       1.000000    70.000000   \n",
       "\n",
       "            BPMeds  prevalentStroke  prevalentHyp     diabetes      totChol  \\\n",
       "count  4187.000000      4240.000000   4240.000000  4240.000000  4190.000000   \n",
       "mean      0.029615         0.005896      0.310613     0.025708   236.699523   \n",
       "std       0.169544         0.076569      0.462799     0.158280    44.591284   \n",
       "min       0.000000         0.000000      0.000000     0.000000   107.000000   \n",
       "25%       0.000000         0.000000      0.000000     0.000000   206.000000   \n",
       "50%       0.000000         0.000000      0.000000     0.000000   234.000000   \n",
       "75%       0.000000         0.000000      1.000000     0.000000   263.000000   \n",
       "max       1.000000         1.000000      1.000000     1.000000   696.000000   \n",
       "\n",
       "             sysBP        diaBP          BMI    heartRate      glucose  \\\n",
       "count  4240.000000  4240.000000  4221.000000  4239.000000  3852.000000   \n",
       "mean    132.354599    82.897759    25.800801    75.878981    81.963655   \n",
       "std      22.033300    11.910394     4.079840    12.025348    23.954335   \n",
       "min      83.500000    48.000000    15.540000    44.000000    40.000000   \n",
       "25%     117.000000    75.000000    23.070000    68.000000    71.000000   \n",
       "50%     128.000000    82.000000    25.400000    75.000000    78.000000   \n",
       "75%     144.000000    90.000000    28.040000    83.000000    87.000000   \n",
       "max     295.000000   142.500000    56.800000   143.000000   394.000000   \n",
       "\n",
       "        TenYearCHD  \n",
       "count  4240.000000  \n",
       "mean      0.151887  \n",
       "std       0.358953  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1682440742567,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "YDytY4xleEvc",
    "outputId": "30c9c8ed-7708-4a76-d99d-ab6e3a0c6c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Values from the whole data set are:  645\n",
      "Number of Columns with Missing Values are:  7\n",
      "Names of the Columns with Missing Values are:  Index(['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate',\n",
      "       'glucose'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(f'Total Missing Values from the whole data set are: ', data.isnull().values.sum())  # Checking the total Null Values of the whole data set \n",
    "print(f'Number of Columns with Missing Values are: ', len(data.columns[data.isnull().any()])) #No of Columns with Missing Valeus\n",
    "print(f'Names of the Columns with Missing Values are: ', data.columns[data.isnull().any()]) #Checking the Name of the columns with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1682440742567,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "8QcrOzMSflAB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1682440742568,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "SXWa6-lIaZSx"
   },
   "outputs": [],
   "source": [
    "#the columns we want to fill missing values for\n",
    "\n",
    "selected_cols = ['education', 'cigsPerDay', 'BPMeds','totChol','BMI','heartRate','glucose']\n",
    "\n",
    "\n",
    "mean_values = data[selected_cols].mean()\n",
    "\n",
    "# Replace missing and NaN values with mean values for the selected columns\n",
    "data[selected_cols] = data[selected_cols].fillna(mean_values)\n",
    "\n",
    "#df.to_csv('new_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1682440742568,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "4paNUXn4tBOb",
    "outputId": "de3c45ad-533c-4ed8-c038-0395a83926ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4240, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1682440742568,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "ymno5Fpbs0Rl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1682440742569,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "oiiUW0nLtGS1",
    "outputId": "7d15dc0d-40a5-41a9-a49d-c3d639db499f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4240, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1682440742569,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "yx286qzyaZPj",
    "outputId": "ee5c80b3-908e-40e1-d845-717494facf66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "TenYearCHD         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1682440742569,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "R6mPhcT6aZNF",
    "outputId": "dc49fb7f-deb6-4e2a-80c1-7ee363cd98d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Values from the whole data set are:  0\n",
      "Number of Columns with Missing Values are:  0\n",
      "Names of the Columns with Missing Values are:  Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    " # Checking the total Null Values of the whole data set\n",
    "print(f'Total Missing Values from the whole data set are: ', data.isnull().values.sum())\n",
    "  \n",
    "#No of Columns with Missing Valeus\n",
    "print(f'Number of Columns with Missing Values are: ', len(data.columns[data.isnull().any()])) \n",
    "\n",
    "#Checking the Name of the columns with null values\n",
    "print(f'Names of the Columns with Missing Values are: ', data.columns[data.isnull().any()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1682440742569,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "CZeasxvTMc6R",
    "outputId": "0f8e1a3a-005c-4e30-8d3a-7dfedd21d8bd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Values: 4240\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='TenYearCHD',data=data,palette='bright',edgecolor='black')\n",
    "#plt.hist(data[\"TenYearCHD\"]);\n",
    "\n",
    "\n",
    "for i in range(len(data['TenYearCHD'].value_counts())):\n",
    "    count = data['TenYearCHD'].value_counts()[i]\n",
    "    label = count\n",
    "    plt.annotate(label, (i, count), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Total Values:',  data.TenYearCHD.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 1695,
     "status": "ok",
     "timestamp": 1682440744248,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "d5g7BcRnM7OO",
    "outputId": "be4230b0-4353-404e-9c5d-7ddccf1173da"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"TenYearCHD\"].value_counts(normalize=True).plot(kind=\"pie\", legend=True, table=True, figsize=(10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1682440744249,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "jeV1-3CB7jIb"
   },
   "outputs": [],
   "source": [
    "X = data.drop(['TenYearCHD'], axis=1)\n",
    "y = data['TenYearCHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1682440744249,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "IajWscySaxaM"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1682440744249,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "Q8E7uBIybG6O",
    "outputId": "b155fa7e-892a-4e3a-c9a5-e33603d5c2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "(3392, 15)\n",
      "(848, 15)\n",
      "(3392,)\n",
      "(848,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1682440744250,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "6Oc8-c9OTek3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1682440744250,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "Eq4MQjVlbLTq"
   },
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "\n",
    "scaler= StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1682441925993,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "7OyVln1abOxA",
    "outputId": "8b661a07-184b-470d-93fd-fac6ca1f351c"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-11167229222b>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mlocal_valid_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    206\u001b[0m                     \u001b[0;34mf\"Invalid parameter {key!r} for estimator {self}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;34mf\"Valid parameters are: {local_valid_params!r}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'alpha' for estimator GaussianNB(). Valid parameters are: ['priors', 'var_smoothing']."
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create Naive Bayes classifier object and train on the training data\n",
    "#nb_clf = GaussianNB()\n",
    "#nb_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'alpha': [0.1, 1.0, 10.0]}\n",
    "nb_clf = GridSearchCV(GaussianNB(), params, cv=5)\n",
    "nb_clf.fit(X_train, y_train)\n",
    "print(nb_clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1682440744251,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "MTxUUDP-qxp_",
    "outputId": "daf0b12a-0dae-4b74-bf0a-cfecd5f80ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FROM TRAINING DATASET :\n",
      "Accuracy score:  82.22\n",
      "Confusion matrix: \n",
      " [[2674  197]\n",
      " [ 406  115]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8682    0.9314    0.8987      2871\n",
      "           1     0.3686    0.2207    0.2761       521\n",
      "\n",
      "    accuracy                         0.8222      3392\n",
      "   macro avg     0.6184    0.5761    0.5874      3392\n",
      "weighted avg     0.7914    0.8222    0.8030      3392\n",
      "\n",
      "ACCURACY FROM Test DATASET :\n",
      "Accuracy score:  83.37\n",
      "Confusion matrix: \n",
      " [[686  39]\n",
      " [102  21]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8706    0.9462    0.9068       725\n",
      "           1     0.3500    0.1707    0.2295       123\n",
      "\n",
      "    accuracy                         0.8337       848\n",
      "   macro avg     0.6103    0.5585    0.5682       848\n",
      "weighted avg     0.7951    0.8337    0.8086       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "\n",
    "nb_train_pred = nb_clf.predict(X_train)\n",
    "nb_y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "print(\"ACCURACY FROM TRAINING DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_train,nb_train_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_train,nb_train_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_train,nb_train_pred,digits=4))\n",
    "\n",
    "\n",
    "print(\"ACCURACY FROM Test DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_test,nb_y_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_test,nb_y_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_test,nb_y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1682440744251,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "CocmoTJLbOtn",
    "outputId": "18400fa4-9eb7-4514-a2b1-eb47aded9117"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_depth=6, n_estimators=5,\n",
       "                       random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, max_depth=6, n_estimators=5,\n",
       "                       random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=6, n_estimators=5,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(max_depth=6 ,n_estimators=5, criterion = 'entropy', random_state=0)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1682440744251,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "CYRi9_LbTXMH",
    "outputId": "d526ef99-cd53-4eac-d2fb-19fd2e5c781e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FROM TRAINING DATASET :\n",
      "Accuracy score:  85.64\n",
      "Confusion matrix: \n",
      " [[2864    7]\n",
      " [ 480   41]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8565    0.9976    0.9216      2871\n",
      "           1     0.8542    0.0787    0.1441       521\n",
      "\n",
      "    accuracy                         0.8564      3392\n",
      "   macro avg     0.8553    0.5381    0.5329      3392\n",
      "weighted avg     0.8561    0.8564    0.8022      3392\n",
      "\n",
      " ACCURACY FROM TESTING DATASET :\n",
      "Accuracy score:  85.37735849056604\n",
      "Confusion matrix: \n",
      " [[722   3]\n",
      " [121   2]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8565    0.9959    0.9209       725\n",
      "           1     0.4000    0.0163    0.0313       123\n",
      "\n",
      "    accuracy                         0.8538       848\n",
      "   macro avg     0.6282    0.5061    0.4761       848\n",
      "weighted avg     0.7903    0.8538    0.7919       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_rfc_train_pred = rf_clf.predict(X_train)\n",
    "y_rfc_test_pred = rf_clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"ACCURACY FROM TRAINING DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_train,y_rfc_train_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_train,y_rfc_train_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_train,y_rfc_train_pred,digits=4))\n",
    "\n",
    "print(\" ACCURACY FROM TESTING DATASET :\")\n",
    "print(\"Accuracy score: \",accuracy_score(y_test,y_rfc_test_pred)*100)\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_test,y_rfc_test_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_test,y_rfc_test_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1682440744252,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "cvBmie2bcevD"
   },
   "outputs": [],
   "source": [
    "#pred=nb_clf.predict(n)\n",
    "#print(pred)\n",
    "#pred=RFC1.predict(n)\n",
    "#print(pred)\n",
    "#print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 8997,
     "status": "ok",
     "timestamp": 1682440753234,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "AQcXwFV2bTu0"
   },
   "outputs": [],
   "source": [
    "#ANN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1682440753235,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "UqNOsqWZ8jA9"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1682440753236,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "cuMWt6Ik8sOk"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the model\n",
    "ann_clf = keras.Sequential([\n",
    "    keras.layers.Dense(15, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1682440753236,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "plJ3EPf-8tX-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# compile the model\n",
    "ann_clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143434,
     "status": "ok",
     "timestamp": 1682440896636,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "qSZB8csg8tI1",
    "outputId": "e5a5ff38-0793-4a7a-bafe-4d76e8b0d001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.6804 - accuracy: 0.5698 - val_loss: 0.5643 - val_accuracy: 0.8100\n",
      "Epoch 2/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5115 - accuracy: 0.8319 - val_loss: 0.4644 - val_accuracy: 0.8483\n",
      "Epoch 3/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.4513 - accuracy: 0.8463 - val_loss: 0.4227 - val_accuracy: 0.8483\n",
      "Epoch 4/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.4255 - accuracy: 0.8478 - val_loss: 0.4026 - val_accuracy: 0.8483\n",
      "Epoch 5/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.4121 - accuracy: 0.8489 - val_loss: 0.3932 - val_accuracy: 0.8483\n",
      "Epoch 6/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.4038 - accuracy: 0.8489 - val_loss: 0.3875 - val_accuracy: 0.8483\n",
      "Epoch 7/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3988 - accuracy: 0.8504 - val_loss: 0.3850 - val_accuracy: 0.8527\n",
      "Epoch 8/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3945 - accuracy: 0.8515 - val_loss: 0.3823 - val_accuracy: 0.8527\n",
      "Epoch 9/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3918 - accuracy: 0.8511 - val_loss: 0.3800 - val_accuracy: 0.8542\n",
      "Epoch 10/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3895 - accuracy: 0.8504 - val_loss: 0.3792 - val_accuracy: 0.8513\n",
      "Epoch 11/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3875 - accuracy: 0.8511 - val_loss: 0.3781 - val_accuracy: 0.8527\n",
      "Epoch 12/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3863 - accuracy: 0.8507 - val_loss: 0.3773 - val_accuracy: 0.8513\n",
      "Epoch 13/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3850 - accuracy: 0.8515 - val_loss: 0.3762 - val_accuracy: 0.8513\n",
      "Epoch 14/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3837 - accuracy: 0.8515 - val_loss: 0.3754 - val_accuracy: 0.8527\n",
      "Epoch 15/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3826 - accuracy: 0.8522 - val_loss: 0.3742 - val_accuracy: 0.8513\n",
      "Epoch 16/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3819 - accuracy: 0.8518 - val_loss: 0.3742 - val_accuracy: 0.8513\n",
      "Epoch 17/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3804 - accuracy: 0.8515 - val_loss: 0.3743 - val_accuracy: 0.8513\n",
      "Epoch 18/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3802 - accuracy: 0.8504 - val_loss: 0.3733 - val_accuracy: 0.8527\n",
      "Epoch 19/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3791 - accuracy: 0.8522 - val_loss: 0.3735 - val_accuracy: 0.8513\n",
      "Epoch 20/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3779 - accuracy: 0.8529 - val_loss: 0.3735 - val_accuracy: 0.8527\n",
      "Epoch 21/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8522 - val_loss: 0.3728 - val_accuracy: 0.8527\n",
      "Epoch 22/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.8529 - val_loss: 0.3724 - val_accuracy: 0.8542\n",
      "Epoch 23/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8529 - val_loss: 0.3730 - val_accuracy: 0.8557\n",
      "Epoch 24/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3746 - accuracy: 0.8533 - val_loss: 0.3726 - val_accuracy: 0.8557\n",
      "Epoch 25/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3742 - accuracy: 0.8544 - val_loss: 0.3732 - val_accuracy: 0.8527\n",
      "Epoch 26/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3732 - accuracy: 0.8529 - val_loss: 0.3727 - val_accuracy: 0.8542\n",
      "Epoch 27/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3725 - accuracy: 0.8533 - val_loss: 0.3729 - val_accuracy: 0.8542\n",
      "Epoch 28/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8551 - val_loss: 0.3731 - val_accuracy: 0.8542\n",
      "Epoch 29/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8544 - val_loss: 0.3732 - val_accuracy: 0.8557\n",
      "Epoch 30/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3704 - accuracy: 0.8544 - val_loss: 0.3725 - val_accuracy: 0.8557\n",
      "Epoch 31/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3699 - accuracy: 0.8548 - val_loss: 0.3727 - val_accuracy: 0.8557\n",
      "Epoch 32/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8551 - val_loss: 0.3728 - val_accuracy: 0.8557\n",
      "Epoch 33/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3689 - accuracy: 0.8544 - val_loss: 0.3720 - val_accuracy: 0.8557\n",
      "Epoch 34/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3684 - accuracy: 0.8548 - val_loss: 0.3724 - val_accuracy: 0.8542\n",
      "Epoch 35/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8555 - val_loss: 0.3719 - val_accuracy: 0.8557\n",
      "Epoch 36/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8559 - val_loss: 0.3724 - val_accuracy: 0.8571\n",
      "Epoch 37/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8559 - val_loss: 0.3711 - val_accuracy: 0.8557\n",
      "Epoch 38/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8562 - val_loss: 0.3711 - val_accuracy: 0.8542\n",
      "Epoch 39/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8566 - val_loss: 0.3721 - val_accuracy: 0.8557\n",
      "Epoch 40/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3651 - accuracy: 0.8577 - val_loss: 0.3717 - val_accuracy: 0.8571\n",
      "Epoch 41/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8570 - val_loss: 0.3713 - val_accuracy: 0.8571\n",
      "Epoch 42/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8566 - val_loss: 0.3729 - val_accuracy: 0.8571\n",
      "Epoch 43/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8574 - val_loss: 0.3719 - val_accuracy: 0.8557\n",
      "Epoch 44/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3636 - accuracy: 0.8577 - val_loss: 0.3729 - val_accuracy: 0.8557\n",
      "Epoch 45/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3634 - accuracy: 0.8581 - val_loss: 0.3717 - val_accuracy: 0.8557\n",
      "Epoch 46/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3629 - accuracy: 0.8570 - val_loss: 0.3725 - val_accuracy: 0.8527\n",
      "Epoch 47/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3621 - accuracy: 0.8585 - val_loss: 0.3733 - val_accuracy: 0.8557\n",
      "Epoch 48/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3620 - accuracy: 0.8581 - val_loss: 0.3721 - val_accuracy: 0.8557\n",
      "Epoch 49/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8588 - val_loss: 0.3727 - val_accuracy: 0.8542\n",
      "Epoch 50/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3614 - accuracy: 0.8577 - val_loss: 0.3722 - val_accuracy: 0.8557\n",
      "Epoch 51/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.8588 - val_loss: 0.3727 - val_accuracy: 0.8557\n",
      "Epoch 52/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3604 - accuracy: 0.8585 - val_loss: 0.3725 - val_accuracy: 0.8557\n",
      "Epoch 53/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8588 - val_loss: 0.3729 - val_accuracy: 0.8557\n",
      "Epoch 54/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3604 - accuracy: 0.8566 - val_loss: 0.3731 - val_accuracy: 0.8542\n",
      "Epoch 55/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8585 - val_loss: 0.3739 - val_accuracy: 0.8527\n",
      "Epoch 56/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8577 - val_loss: 0.3734 - val_accuracy: 0.8542\n",
      "Epoch 57/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8588 - val_loss: 0.3732 - val_accuracy: 0.8527\n",
      "Epoch 58/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3581 - accuracy: 0.8585 - val_loss: 0.3742 - val_accuracy: 0.8542\n",
      "Epoch 59/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8592 - val_loss: 0.3740 - val_accuracy: 0.8527\n",
      "Epoch 60/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.8574 - val_loss: 0.3743 - val_accuracy: 0.8527\n",
      "Epoch 61/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3571 - accuracy: 0.8592 - val_loss: 0.3755 - val_accuracy: 0.8542\n",
      "Epoch 62/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8596 - val_loss: 0.3737 - val_accuracy: 0.8513\n",
      "Epoch 63/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8596 - val_loss: 0.3739 - val_accuracy: 0.8513\n",
      "Epoch 64/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8603 - val_loss: 0.3749 - val_accuracy: 0.8513\n",
      "Epoch 65/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8614 - val_loss: 0.3753 - val_accuracy: 0.8498\n",
      "Epoch 66/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.8592 - val_loss: 0.3754 - val_accuracy: 0.8513\n",
      "Epoch 67/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3549 - accuracy: 0.8599 - val_loss: 0.3755 - val_accuracy: 0.8513\n",
      "Epoch 68/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8610 - val_loss: 0.3764 - val_accuracy: 0.8527\n",
      "Epoch 69/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3543 - accuracy: 0.8610 - val_loss: 0.3753 - val_accuracy: 0.8513\n",
      "Epoch 70/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8603 - val_loss: 0.3759 - val_accuracy: 0.8513\n",
      "Epoch 71/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3534 - accuracy: 0.8614 - val_loss: 0.3759 - val_accuracy: 0.8513\n",
      "Epoch 72/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8603 - val_loss: 0.3766 - val_accuracy: 0.8513\n",
      "Epoch 73/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8610 - val_loss: 0.3778 - val_accuracy: 0.8542\n",
      "Epoch 74/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 0.8625 - val_loss: 0.3775 - val_accuracy: 0.8498\n",
      "Epoch 75/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3520 - accuracy: 0.8614 - val_loss: 0.3770 - val_accuracy: 0.8498\n",
      "Epoch 76/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8603 - val_loss: 0.3781 - val_accuracy: 0.8513\n",
      "Epoch 77/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8633 - val_loss: 0.3778 - val_accuracy: 0.8498\n",
      "Epoch 78/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8603 - val_loss: 0.3780 - val_accuracy: 0.8498\n",
      "Epoch 79/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8618 - val_loss: 0.3785 - val_accuracy: 0.8498\n",
      "Epoch 80/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3504 - accuracy: 0.8629 - val_loss: 0.3786 - val_accuracy: 0.8468\n",
      "Epoch 81/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8610 - val_loss: 0.3793 - val_accuracy: 0.8498\n",
      "Epoch 82/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8618 - val_loss: 0.3794 - val_accuracy: 0.8468\n",
      "Epoch 83/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8625 - val_loss: 0.3794 - val_accuracy: 0.8468\n",
      "Epoch 84/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3487 - accuracy: 0.8625 - val_loss: 0.3811 - val_accuracy: 0.8498\n",
      "Epoch 85/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3485 - accuracy: 0.8636 - val_loss: 0.3798 - val_accuracy: 0.8483\n",
      "Epoch 86/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3481 - accuracy: 0.8614 - val_loss: 0.3807 - val_accuracy: 0.8468\n",
      "Epoch 87/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3476 - accuracy: 0.8629 - val_loss: 0.3811 - val_accuracy: 0.8454\n",
      "Epoch 88/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3476 - accuracy: 0.8625 - val_loss: 0.3819 - val_accuracy: 0.8468\n",
      "Epoch 89/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3470 - accuracy: 0.8633 - val_loss: 0.3822 - val_accuracy: 0.8439\n",
      "Epoch 90/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3474 - accuracy: 0.8633 - val_loss: 0.3829 - val_accuracy: 0.8454\n",
      "Epoch 91/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3471 - accuracy: 0.8629 - val_loss: 0.3829 - val_accuracy: 0.8424\n",
      "Epoch 92/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3467 - accuracy: 0.8618 - val_loss: 0.3825 - val_accuracy: 0.8439\n",
      "Epoch 93/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3457 - accuracy: 0.8614 - val_loss: 0.3841 - val_accuracy: 0.8454\n",
      "Epoch 94/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3454 - accuracy: 0.8625 - val_loss: 0.3839 - val_accuracy: 0.8454\n",
      "Epoch 95/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3452 - accuracy: 0.8640 - val_loss: 0.3844 - val_accuracy: 0.8439\n",
      "Epoch 96/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8621 - val_loss: 0.3858 - val_accuracy: 0.8454\n",
      "Epoch 97/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 0.8629 - val_loss: 0.3847 - val_accuracy: 0.8424\n",
      "Epoch 98/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8633 - val_loss: 0.3860 - val_accuracy: 0.8409\n",
      "Epoch 99/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8640 - val_loss: 0.3860 - val_accuracy: 0.8439\n",
      "Epoch 100/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8636 - val_loss: 0.3857 - val_accuracy: 0.8439\n",
      "Epoch 101/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8640 - val_loss: 0.3882 - val_accuracy: 0.8409\n",
      "Epoch 102/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8629 - val_loss: 0.3878 - val_accuracy: 0.8424\n",
      "Epoch 103/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8636 - val_loss: 0.3879 - val_accuracy: 0.8424\n",
      "Epoch 104/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.8625 - val_loss: 0.3872 - val_accuracy: 0.8409\n",
      "Epoch 105/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8640 - val_loss: 0.3878 - val_accuracy: 0.8439\n",
      "Epoch 106/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8625 - val_loss: 0.3889 - val_accuracy: 0.8409\n",
      "Epoch 107/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8633 - val_loss: 0.3890 - val_accuracy: 0.8454\n",
      "Epoch 108/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8640 - val_loss: 0.3890 - val_accuracy: 0.8454\n",
      "Epoch 109/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3410 - accuracy: 0.8614 - val_loss: 0.3886 - val_accuracy: 0.8409\n",
      "Epoch 110/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3411 - accuracy: 0.8647 - val_loss: 0.3896 - val_accuracy: 0.8439\n",
      "Epoch 111/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8621 - val_loss: 0.3901 - val_accuracy: 0.8409\n",
      "Epoch 112/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8640 - val_loss: 0.3899 - val_accuracy: 0.8439\n",
      "Epoch 113/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8647 - val_loss: 0.3905 - val_accuracy: 0.8424\n",
      "Epoch 114/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8640 - val_loss: 0.3908 - val_accuracy: 0.8424\n",
      "Epoch 115/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3386 - accuracy: 0.8651 - val_loss: 0.3911 - val_accuracy: 0.8409\n",
      "Epoch 116/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3391 - accuracy: 0.8651 - val_loss: 0.3907 - val_accuracy: 0.8424\n",
      "Epoch 117/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3395 - accuracy: 0.8629 - val_loss: 0.3917 - val_accuracy: 0.8395\n",
      "Epoch 118/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8644 - val_loss: 0.3918 - val_accuracy: 0.8439\n",
      "Epoch 119/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8647 - val_loss: 0.3929 - val_accuracy: 0.8424\n",
      "Epoch 120/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3377 - accuracy: 0.8673 - val_loss: 0.3928 - val_accuracy: 0.8380\n",
      "Epoch 121/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8640 - val_loss: 0.3922 - val_accuracy: 0.8365\n",
      "Epoch 122/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8669 - val_loss: 0.3920 - val_accuracy: 0.8365\n",
      "Epoch 123/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3371 - accuracy: 0.8640 - val_loss: 0.3926 - val_accuracy: 0.8380\n",
      "Epoch 124/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8669 - val_loss: 0.3935 - val_accuracy: 0.8380\n",
      "Epoch 125/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8651 - val_loss: 0.3945 - val_accuracy: 0.8380\n",
      "Epoch 126/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8640 - val_loss: 0.3950 - val_accuracy: 0.8380\n",
      "Epoch 127/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3356 - accuracy: 0.8658 - val_loss: 0.3941 - val_accuracy: 0.8380\n",
      "Epoch 128/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8647 - val_loss: 0.3949 - val_accuracy: 0.8365\n",
      "Epoch 129/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8640 - val_loss: 0.3944 - val_accuracy: 0.8380\n",
      "Epoch 130/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8647 - val_loss: 0.3951 - val_accuracy: 0.8395\n",
      "Epoch 131/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8644 - val_loss: 0.3959 - val_accuracy: 0.8351\n",
      "Epoch 132/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8651 - val_loss: 0.3960 - val_accuracy: 0.8380\n",
      "Epoch 133/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8658 - val_loss: 0.3968 - val_accuracy: 0.8351\n",
      "Epoch 134/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8669 - val_loss: 0.3964 - val_accuracy: 0.8351\n",
      "Epoch 135/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8644 - val_loss: 0.3976 - val_accuracy: 0.8351\n",
      "Epoch 136/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3334 - accuracy: 0.8651 - val_loss: 0.3972 - val_accuracy: 0.8351\n",
      "Epoch 137/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3332 - accuracy: 0.8644 - val_loss: 0.3980 - val_accuracy: 0.8351\n",
      "Epoch 138/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.8644 - val_loss: 0.3977 - val_accuracy: 0.8321\n",
      "Epoch 139/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.8644 - val_loss: 0.3984 - val_accuracy: 0.8321\n",
      "Epoch 140/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.8662 - val_loss: 0.3989 - val_accuracy: 0.8336\n",
      "Epoch 141/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8651 - val_loss: 0.3993 - val_accuracy: 0.8351\n",
      "Epoch 142/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8662 - val_loss: 0.3993 - val_accuracy: 0.8351\n",
      "Epoch 143/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8647 - val_loss: 0.3984 - val_accuracy: 0.8351\n",
      "Epoch 144/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3327 - accuracy: 0.8655 - val_loss: 0.4001 - val_accuracy: 0.8351\n",
      "Epoch 145/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3319 - accuracy: 0.8669 - val_loss: 0.3999 - val_accuracy: 0.8321\n",
      "Epoch 146/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3309 - accuracy: 0.8647 - val_loss: 0.4010 - val_accuracy: 0.8351\n",
      "Epoch 147/450\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3306 - accuracy: 0.8666 - val_loss: 0.4005 - val_accuracy: 0.8321\n",
      "Epoch 148/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3307 - accuracy: 0.8658 - val_loss: 0.4015 - val_accuracy: 0.8336\n",
      "Epoch 149/450\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3304 - accuracy: 0.8673 - val_loss: 0.4017 - val_accuracy: 0.8351\n",
      "Epoch 150/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3300 - accuracy: 0.8666 - val_loss: 0.4018 - val_accuracy: 0.8321\n",
      "Epoch 151/450\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3306 - accuracy: 0.8658 - val_loss: 0.4015 - val_accuracy: 0.8336\n",
      "Epoch 152/450\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.3297 - accuracy: 0.8684 - val_loss: 0.4029 - val_accuracy: 0.8351\n",
      "Epoch 153/450\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3292 - accuracy: 0.8669 - val_loss: 0.4032 - val_accuracy: 0.8321\n",
      "Epoch 154/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3298 - accuracy: 0.8644 - val_loss: 0.4025 - val_accuracy: 0.8306\n",
      "Epoch 155/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3293 - accuracy: 0.8640 - val_loss: 0.4043 - val_accuracy: 0.8336\n",
      "Epoch 156/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3286 - accuracy: 0.8677 - val_loss: 0.4064 - val_accuracy: 0.8321\n",
      "Epoch 157/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3279 - accuracy: 0.8684 - val_loss: 0.4049 - val_accuracy: 0.8321\n",
      "Epoch 158/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3281 - accuracy: 0.8666 - val_loss: 0.4061 - val_accuracy: 0.8321\n",
      "Epoch 159/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3283 - accuracy: 0.8658 - val_loss: 0.4055 - val_accuracy: 0.8336\n",
      "Epoch 160/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3279 - accuracy: 0.8688 - val_loss: 0.4064 - val_accuracy: 0.8306\n",
      "Epoch 161/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3272 - accuracy: 0.8684 - val_loss: 0.4058 - val_accuracy: 0.8351\n",
      "Epoch 162/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3274 - accuracy: 0.8691 - val_loss: 0.4069 - val_accuracy: 0.8351\n",
      "Epoch 163/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3272 - accuracy: 0.8669 - val_loss: 0.4065 - val_accuracy: 0.8336\n",
      "Epoch 164/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3268 - accuracy: 0.8695 - val_loss: 0.4087 - val_accuracy: 0.8351\n",
      "Epoch 165/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3273 - accuracy: 0.8680 - val_loss: 0.4088 - val_accuracy: 0.8306\n",
      "Epoch 166/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3262 - accuracy: 0.8699 - val_loss: 0.4083 - val_accuracy: 0.8351\n",
      "Epoch 167/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3267 - accuracy: 0.8684 - val_loss: 0.4097 - val_accuracy: 0.8321\n",
      "Epoch 168/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3263 - accuracy: 0.8699 - val_loss: 0.4095 - val_accuracy: 0.8336\n",
      "Epoch 169/450\n",
      "48/48 [==============================] - 1s 10ms/step - loss: 0.3262 - accuracy: 0.8680 - val_loss: 0.4086 - val_accuracy: 0.8321\n",
      "Epoch 170/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3253 - accuracy: 0.8710 - val_loss: 0.4100 - val_accuracy: 0.8321\n",
      "Epoch 171/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3253 - accuracy: 0.8703 - val_loss: 0.4100 - val_accuracy: 0.8321\n",
      "Epoch 172/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3252 - accuracy: 0.8706 - val_loss: 0.4118 - val_accuracy: 0.8321\n",
      "Epoch 173/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3248 - accuracy: 0.8699 - val_loss: 0.4122 - val_accuracy: 0.8351\n",
      "Epoch 174/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3247 - accuracy: 0.8717 - val_loss: 0.4128 - val_accuracy: 0.8321\n",
      "Epoch 175/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3245 - accuracy: 0.8677 - val_loss: 0.4130 - val_accuracy: 0.8321\n",
      "Epoch 176/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3243 - accuracy: 0.8684 - val_loss: 0.4135 - val_accuracy: 0.8321\n",
      "Epoch 177/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3251 - accuracy: 0.8695 - val_loss: 0.4151 - val_accuracy: 0.8321\n",
      "Epoch 178/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3246 - accuracy: 0.8732 - val_loss: 0.4168 - val_accuracy: 0.8321\n",
      "Epoch 179/450\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.3245 - accuracy: 0.8710 - val_loss: 0.4131 - val_accuracy: 0.8321\n",
      "Epoch 180/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3235 - accuracy: 0.8710 - val_loss: 0.4143 - val_accuracy: 0.8321\n",
      "Epoch 181/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3234 - accuracy: 0.8714 - val_loss: 0.4155 - val_accuracy: 0.8306\n",
      "Epoch 182/450\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.3230 - accuracy: 0.8710 - val_loss: 0.4147 - val_accuracy: 0.8321\n",
      "Epoch 183/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3228 - accuracy: 0.8703 - val_loss: 0.4162 - val_accuracy: 0.8321\n",
      "Epoch 184/450\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.3230 - accuracy: 0.8706 - val_loss: 0.4155 - val_accuracy: 0.8306\n",
      "Epoch 185/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3225 - accuracy: 0.8691 - val_loss: 0.4176 - val_accuracy: 0.8306\n",
      "Epoch 186/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8699 - val_loss: 0.4163 - val_accuracy: 0.8321\n",
      "Epoch 187/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3224 - accuracy: 0.8714 - val_loss: 0.4174 - val_accuracy: 0.8306\n",
      "Epoch 188/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8714 - val_loss: 0.4167 - val_accuracy: 0.8321\n",
      "Epoch 189/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8728 - val_loss: 0.4177 - val_accuracy: 0.8321\n",
      "Epoch 190/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3213 - accuracy: 0.8688 - val_loss: 0.4184 - val_accuracy: 0.8292\n",
      "Epoch 191/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3209 - accuracy: 0.8736 - val_loss: 0.4193 - val_accuracy: 0.8321\n",
      "Epoch 192/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3208 - accuracy: 0.8725 - val_loss: 0.4187 - val_accuracy: 0.8321\n",
      "Epoch 193/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8710 - val_loss: 0.4208 - val_accuracy: 0.8306\n",
      "Epoch 194/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.8739 - val_loss: 0.4229 - val_accuracy: 0.8336\n",
      "Epoch 195/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.8714 - val_loss: 0.4207 - val_accuracy: 0.8306\n",
      "Epoch 196/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8728 - val_loss: 0.4204 - val_accuracy: 0.8292\n",
      "Epoch 197/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8717 - val_loss: 0.4202 - val_accuracy: 0.8321\n",
      "Epoch 198/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8736 - val_loss: 0.4205 - val_accuracy: 0.8292\n",
      "Epoch 199/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8710 - val_loss: 0.4216 - val_accuracy: 0.8306\n",
      "Epoch 200/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8721 - val_loss: 0.4221 - val_accuracy: 0.8321\n",
      "Epoch 201/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8725 - val_loss: 0.4223 - val_accuracy: 0.8321\n",
      "Epoch 202/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3191 - accuracy: 0.8736 - val_loss: 0.4230 - val_accuracy: 0.8292\n",
      "Epoch 203/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8717 - val_loss: 0.4238 - val_accuracy: 0.8321\n",
      "Epoch 204/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8739 - val_loss: 0.4242 - val_accuracy: 0.8292\n",
      "Epoch 205/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8721 - val_loss: 0.4257 - val_accuracy: 0.8292\n",
      "Epoch 206/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8725 - val_loss: 0.4268 - val_accuracy: 0.8321\n",
      "Epoch 207/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8710 - val_loss: 0.4256 - val_accuracy: 0.8292\n",
      "Epoch 208/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8728 - val_loss: 0.4265 - val_accuracy: 0.8321\n",
      "Epoch 209/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8721 - val_loss: 0.4258 - val_accuracy: 0.8292\n",
      "Epoch 210/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.8710 - val_loss: 0.4268 - val_accuracy: 0.8292\n",
      "Epoch 211/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8743 - val_loss: 0.4274 - val_accuracy: 0.8262\n",
      "Epoch 212/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8721 - val_loss: 0.4272 - val_accuracy: 0.8292\n",
      "Epoch 213/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8732 - val_loss: 0.4286 - val_accuracy: 0.8292\n",
      "Epoch 214/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3168 - accuracy: 0.8725 - val_loss: 0.4284 - val_accuracy: 0.8292\n",
      "Epoch 215/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8739 - val_loss: 0.4273 - val_accuracy: 0.8306\n",
      "Epoch 216/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8754 - val_loss: 0.4279 - val_accuracy: 0.8306\n",
      "Epoch 217/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3160 - accuracy: 0.8754 - val_loss: 0.4285 - val_accuracy: 0.8306\n",
      "Epoch 218/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3158 - accuracy: 0.8743 - val_loss: 0.4282 - val_accuracy: 0.8321\n",
      "Epoch 219/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3152 - accuracy: 0.8736 - val_loss: 0.4291 - val_accuracy: 0.8277\n",
      "Epoch 220/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3162 - accuracy: 0.8732 - val_loss: 0.4298 - val_accuracy: 0.8277\n",
      "Epoch 221/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.8743 - val_loss: 0.4305 - val_accuracy: 0.8292\n",
      "Epoch 222/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8728 - val_loss: 0.4282 - val_accuracy: 0.8292\n",
      "Epoch 223/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8739 - val_loss: 0.4289 - val_accuracy: 0.8262\n",
      "Epoch 224/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8747 - val_loss: 0.4311 - val_accuracy: 0.8292\n",
      "Epoch 225/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8747 - val_loss: 0.4290 - val_accuracy: 0.8277\n",
      "Epoch 226/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8747 - val_loss: 0.4302 - val_accuracy: 0.8292\n",
      "Epoch 227/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8747 - val_loss: 0.4303 - val_accuracy: 0.8277\n",
      "Epoch 228/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3134 - accuracy: 0.8747 - val_loss: 0.4319 - val_accuracy: 0.8292\n",
      "Epoch 229/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8765 - val_loss: 0.4317 - val_accuracy: 0.8277\n",
      "Epoch 230/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8732 - val_loss: 0.4329 - val_accuracy: 0.8262\n",
      "Epoch 231/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3136 - accuracy: 0.8739 - val_loss: 0.4331 - val_accuracy: 0.8292\n",
      "Epoch 232/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3124 - accuracy: 0.8791 - val_loss: 0.4328 - val_accuracy: 0.8247\n",
      "Epoch 233/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3122 - accuracy: 0.8750 - val_loss: 0.4341 - val_accuracy: 0.8262\n",
      "Epoch 234/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3122 - accuracy: 0.8765 - val_loss: 0.4349 - val_accuracy: 0.8233\n",
      "Epoch 235/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3121 - accuracy: 0.8754 - val_loss: 0.4341 - val_accuracy: 0.8277\n",
      "Epoch 236/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3121 - accuracy: 0.8769 - val_loss: 0.4368 - val_accuracy: 0.8262\n",
      "Epoch 237/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3119 - accuracy: 0.8750 - val_loss: 0.4338 - val_accuracy: 0.8306\n",
      "Epoch 238/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3116 - accuracy: 0.8765 - val_loss: 0.4344 - val_accuracy: 0.8277\n",
      "Epoch 239/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3121 - accuracy: 0.8739 - val_loss: 0.4338 - val_accuracy: 0.8262\n",
      "Epoch 240/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3110 - accuracy: 0.8754 - val_loss: 0.4375 - val_accuracy: 0.8277\n",
      "Epoch 241/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3113 - accuracy: 0.8758 - val_loss: 0.4363 - val_accuracy: 0.8262\n",
      "Epoch 242/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3109 - accuracy: 0.8765 - val_loss: 0.4367 - val_accuracy: 0.8247\n",
      "Epoch 243/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3107 - accuracy: 0.8769 - val_loss: 0.4370 - val_accuracy: 0.8277\n",
      "Epoch 244/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3107 - accuracy: 0.8758 - val_loss: 0.4364 - val_accuracy: 0.8247\n",
      "Epoch 245/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3104 - accuracy: 0.8780 - val_loss: 0.4369 - val_accuracy: 0.8277\n",
      "Epoch 246/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3102 - accuracy: 0.8750 - val_loss: 0.4394 - val_accuracy: 0.8277\n",
      "Epoch 247/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8769 - val_loss: 0.4382 - val_accuracy: 0.8262\n",
      "Epoch 248/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3094 - accuracy: 0.8791 - val_loss: 0.4384 - val_accuracy: 0.8262\n",
      "Epoch 249/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3097 - accuracy: 0.8784 - val_loss: 0.4408 - val_accuracy: 0.8277\n",
      "Epoch 250/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3091 - accuracy: 0.8776 - val_loss: 0.4405 - val_accuracy: 0.8277\n",
      "Epoch 251/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3089 - accuracy: 0.8776 - val_loss: 0.4389 - val_accuracy: 0.8277\n",
      "Epoch 252/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3088 - accuracy: 0.8798 - val_loss: 0.4402 - val_accuracy: 0.8277\n",
      "Epoch 253/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3084 - accuracy: 0.8795 - val_loss: 0.4403 - val_accuracy: 0.8247\n",
      "Epoch 254/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3086 - accuracy: 0.8784 - val_loss: 0.4409 - val_accuracy: 0.8262\n",
      "Epoch 255/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3080 - accuracy: 0.8773 - val_loss: 0.4395 - val_accuracy: 0.8262\n",
      "Epoch 256/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3079 - accuracy: 0.8802 - val_loss: 0.4422 - val_accuracy: 0.8277\n",
      "Epoch 257/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3080 - accuracy: 0.8780 - val_loss: 0.4429 - val_accuracy: 0.8277\n",
      "Epoch 258/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3079 - accuracy: 0.8780 - val_loss: 0.4431 - val_accuracy: 0.8262\n",
      "Epoch 259/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3077 - accuracy: 0.8776 - val_loss: 0.4438 - val_accuracy: 0.8277\n",
      "Epoch 260/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3080 - accuracy: 0.8802 - val_loss: 0.4449 - val_accuracy: 0.8247\n",
      "Epoch 261/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3078 - accuracy: 0.8776 - val_loss: 0.4438 - val_accuracy: 0.8292\n",
      "Epoch 262/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3082 - accuracy: 0.8780 - val_loss: 0.4446 - val_accuracy: 0.8262\n",
      "Epoch 263/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3074 - accuracy: 0.8769 - val_loss: 0.4442 - val_accuracy: 0.8247\n",
      "Epoch 264/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3068 - accuracy: 0.8784 - val_loss: 0.4456 - val_accuracy: 0.8233\n",
      "Epoch 265/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3068 - accuracy: 0.8798 - val_loss: 0.4446 - val_accuracy: 0.8262\n",
      "Epoch 266/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3069 - accuracy: 0.8802 - val_loss: 0.4466 - val_accuracy: 0.8247\n",
      "Epoch 267/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3068 - accuracy: 0.8798 - val_loss: 0.4446 - val_accuracy: 0.8247\n",
      "Epoch 268/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3063 - accuracy: 0.8791 - val_loss: 0.4468 - val_accuracy: 0.8218\n",
      "Epoch 269/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3060 - accuracy: 0.8813 - val_loss: 0.4468 - val_accuracy: 0.8262\n",
      "Epoch 270/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3059 - accuracy: 0.8784 - val_loss: 0.4481 - val_accuracy: 0.8233\n",
      "Epoch 271/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3053 - accuracy: 0.8798 - val_loss: 0.4477 - val_accuracy: 0.8247\n",
      "Epoch 272/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3055 - accuracy: 0.8795 - val_loss: 0.4476 - val_accuracy: 0.8233\n",
      "Epoch 273/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8795 - val_loss: 0.4473 - val_accuracy: 0.8233\n",
      "Epoch 274/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3054 - accuracy: 0.8787 - val_loss: 0.4476 - val_accuracy: 0.8233\n",
      "Epoch 275/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3063 - accuracy: 0.8784 - val_loss: 0.4472 - val_accuracy: 0.8233\n",
      "Epoch 276/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3051 - accuracy: 0.8795 - val_loss: 0.4495 - val_accuracy: 0.8218\n",
      "Epoch 277/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3049 - accuracy: 0.8806 - val_loss: 0.4509 - val_accuracy: 0.8233\n",
      "Epoch 278/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.8784 - val_loss: 0.4507 - val_accuracy: 0.8233\n",
      "Epoch 279/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.8773 - val_loss: 0.4509 - val_accuracy: 0.8233\n",
      "Epoch 280/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.8787 - val_loss: 0.4519 - val_accuracy: 0.8218\n",
      "Epoch 281/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8798 - val_loss: 0.4514 - val_accuracy: 0.8233\n",
      "Epoch 282/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8787 - val_loss: 0.4521 - val_accuracy: 0.8233\n",
      "Epoch 283/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3041 - accuracy: 0.8795 - val_loss: 0.4521 - val_accuracy: 0.8218\n",
      "Epoch 284/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3040 - accuracy: 0.8784 - val_loss: 0.4532 - val_accuracy: 0.8233\n",
      "Epoch 285/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3035 - accuracy: 0.8798 - val_loss: 0.4519 - val_accuracy: 0.8218\n",
      "Epoch 286/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3032 - accuracy: 0.8798 - val_loss: 0.4537 - val_accuracy: 0.8203\n",
      "Epoch 287/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3035 - accuracy: 0.8787 - val_loss: 0.4544 - val_accuracy: 0.8218\n",
      "Epoch 288/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.8787 - val_loss: 0.4560 - val_accuracy: 0.8203\n",
      "Epoch 289/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3029 - accuracy: 0.8787 - val_loss: 0.4555 - val_accuracy: 0.8233\n",
      "Epoch 290/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3024 - accuracy: 0.8798 - val_loss: 0.4576 - val_accuracy: 0.8247\n",
      "Epoch 291/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3028 - accuracy: 0.8802 - val_loss: 0.4565 - val_accuracy: 0.8233\n",
      "Epoch 292/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3028 - accuracy: 0.8787 - val_loss: 0.4574 - val_accuracy: 0.8203\n",
      "Epoch 293/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8795 - val_loss: 0.4566 - val_accuracy: 0.8233\n",
      "Epoch 294/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3027 - accuracy: 0.8780 - val_loss: 0.4569 - val_accuracy: 0.8233\n",
      "Epoch 295/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3023 - accuracy: 0.8791 - val_loss: 0.4571 - val_accuracy: 0.8233\n",
      "Epoch 296/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3018 - accuracy: 0.8817 - val_loss: 0.4624 - val_accuracy: 0.8218\n",
      "Epoch 297/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3015 - accuracy: 0.8784 - val_loss: 0.4601 - val_accuracy: 0.8218\n",
      "Epoch 298/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8787 - val_loss: 0.4595 - val_accuracy: 0.8218\n",
      "Epoch 299/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3016 - accuracy: 0.8809 - val_loss: 0.4605 - val_accuracy: 0.8218\n",
      "Epoch 300/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3019 - accuracy: 0.8798 - val_loss: 0.4616 - val_accuracy: 0.8233\n",
      "Epoch 301/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3011 - accuracy: 0.8802 - val_loss: 0.4634 - val_accuracy: 0.8247\n",
      "Epoch 302/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8802 - val_loss: 0.4622 - val_accuracy: 0.8203\n",
      "Epoch 303/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3016 - accuracy: 0.8802 - val_loss: 0.4654 - val_accuracy: 0.8247\n",
      "Epoch 304/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3013 - accuracy: 0.8798 - val_loss: 0.4645 - val_accuracy: 0.8218\n",
      "Epoch 305/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3001 - accuracy: 0.8806 - val_loss: 0.4631 - val_accuracy: 0.8189\n",
      "Epoch 306/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3008 - accuracy: 0.8795 - val_loss: 0.4638 - val_accuracy: 0.8218\n",
      "Epoch 307/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3004 - accuracy: 0.8802 - val_loss: 0.4651 - val_accuracy: 0.8218\n",
      "Epoch 308/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8802 - val_loss: 0.4681 - val_accuracy: 0.8247\n",
      "Epoch 309/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3006 - accuracy: 0.8787 - val_loss: 0.4673 - val_accuracy: 0.8218\n",
      "Epoch 310/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3001 - accuracy: 0.8806 - val_loss: 0.4670 - val_accuracy: 0.8233\n",
      "Epoch 311/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3002 - accuracy: 0.8795 - val_loss: 0.4650 - val_accuracy: 0.8233\n",
      "Epoch 312/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3003 - accuracy: 0.8773 - val_loss: 0.4665 - val_accuracy: 0.8247\n",
      "Epoch 313/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2995 - accuracy: 0.8791 - val_loss: 0.4697 - val_accuracy: 0.8218\n",
      "Epoch 314/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3002 - accuracy: 0.8802 - val_loss: 0.4678 - val_accuracy: 0.8233\n",
      "Epoch 315/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2995 - accuracy: 0.8795 - val_loss: 0.4682 - val_accuracy: 0.8233\n",
      "Epoch 316/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2997 - accuracy: 0.8784 - val_loss: 0.4691 - val_accuracy: 0.8218\n",
      "Epoch 317/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2997 - accuracy: 0.8813 - val_loss: 0.4699 - val_accuracy: 0.8218\n",
      "Epoch 318/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2991 - accuracy: 0.8787 - val_loss: 0.4698 - val_accuracy: 0.8233\n",
      "Epoch 319/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2994 - accuracy: 0.8787 - val_loss: 0.4685 - val_accuracy: 0.8203\n",
      "Epoch 320/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2990 - accuracy: 0.8824 - val_loss: 0.4713 - val_accuracy: 0.8233\n",
      "Epoch 321/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2986 - accuracy: 0.8809 - val_loss: 0.4716 - val_accuracy: 0.8189\n",
      "Epoch 322/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3000 - accuracy: 0.8791 - val_loss: 0.4707 - val_accuracy: 0.8174\n",
      "Epoch 323/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2990 - accuracy: 0.8791 - val_loss: 0.4712 - val_accuracy: 0.8174\n",
      "Epoch 324/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2986 - accuracy: 0.8798 - val_loss: 0.4711 - val_accuracy: 0.8203\n",
      "Epoch 325/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2986 - accuracy: 0.8798 - val_loss: 0.4709 - val_accuracy: 0.8174\n",
      "Epoch 326/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2988 - accuracy: 0.8791 - val_loss: 0.4725 - val_accuracy: 0.8203\n",
      "Epoch 327/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2990 - accuracy: 0.8795 - val_loss: 0.4731 - val_accuracy: 0.8189\n",
      "Epoch 328/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2988 - accuracy: 0.8813 - val_loss: 0.4721 - val_accuracy: 0.8218\n",
      "Epoch 329/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2980 - accuracy: 0.8780 - val_loss: 0.4733 - val_accuracy: 0.8203\n",
      "Epoch 330/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2982 - accuracy: 0.8798 - val_loss: 0.4729 - val_accuracy: 0.8203\n",
      "Epoch 331/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.8813 - val_loss: 0.4733 - val_accuracy: 0.8218\n",
      "Epoch 332/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.8791 - val_loss: 0.4746 - val_accuracy: 0.8233\n",
      "Epoch 333/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8806 - val_loss: 0.4738 - val_accuracy: 0.8203\n",
      "Epoch 334/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2981 - accuracy: 0.8798 - val_loss: 0.4738 - val_accuracy: 0.8247\n",
      "Epoch 335/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2980 - accuracy: 0.8809 - val_loss: 0.4755 - val_accuracy: 0.8233\n",
      "Epoch 336/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8791 - val_loss: 0.4763 - val_accuracy: 0.8218\n",
      "Epoch 337/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2979 - accuracy: 0.8809 - val_loss: 0.4770 - val_accuracy: 0.8203\n",
      "Epoch 338/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2982 - accuracy: 0.8817 - val_loss: 0.4759 - val_accuracy: 0.8247\n",
      "Epoch 339/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2976 - accuracy: 0.8824 - val_loss: 0.4767 - val_accuracy: 0.8233\n",
      "Epoch 340/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2973 - accuracy: 0.8795 - val_loss: 0.4760 - val_accuracy: 0.8203\n",
      "Epoch 341/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2975 - accuracy: 0.8817 - val_loss: 0.4767 - val_accuracy: 0.8247\n",
      "Epoch 342/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8809 - val_loss: 0.4756 - val_accuracy: 0.8203\n",
      "Epoch 343/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2976 - accuracy: 0.8820 - val_loss: 0.4761 - val_accuracy: 0.8233\n",
      "Epoch 344/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.8832 - val_loss: 0.4764 - val_accuracy: 0.8174\n",
      "Epoch 345/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2965 - accuracy: 0.8802 - val_loss: 0.4764 - val_accuracy: 0.8218\n",
      "Epoch 346/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2982 - accuracy: 0.8813 - val_loss: 0.4755 - val_accuracy: 0.8233\n",
      "Epoch 347/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2969 - accuracy: 0.8820 - val_loss: 0.4760 - val_accuracy: 0.8203\n",
      "Epoch 348/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2974 - accuracy: 0.8802 - val_loss: 0.4769 - val_accuracy: 0.8189\n",
      "Epoch 349/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2971 - accuracy: 0.8809 - val_loss: 0.4810 - val_accuracy: 0.8277\n",
      "Epoch 350/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2974 - accuracy: 0.8795 - val_loss: 0.4787 - val_accuracy: 0.8262\n",
      "Epoch 351/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2962 - accuracy: 0.8817 - val_loss: 0.4773 - val_accuracy: 0.8218\n",
      "Epoch 352/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8813 - val_loss: 0.4793 - val_accuracy: 0.8262\n",
      "Epoch 353/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2960 - accuracy: 0.8802 - val_loss: 0.4785 - val_accuracy: 0.8262\n",
      "Epoch 354/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2958 - accuracy: 0.8806 - val_loss: 0.4797 - val_accuracy: 0.8321\n",
      "Epoch 355/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.8791 - val_loss: 0.4797 - val_accuracy: 0.8277\n",
      "Epoch 356/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2963 - accuracy: 0.8817 - val_loss: 0.4784 - val_accuracy: 0.8233\n",
      "Epoch 357/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2959 - accuracy: 0.8809 - val_loss: 0.4792 - val_accuracy: 0.8247\n",
      "Epoch 358/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2954 - accuracy: 0.8802 - val_loss: 0.4800 - val_accuracy: 0.8218\n",
      "Epoch 359/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2956 - accuracy: 0.8791 - val_loss: 0.4799 - val_accuracy: 0.8203\n",
      "Epoch 360/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.8791 - val_loss: 0.4811 - val_accuracy: 0.8306\n",
      "Epoch 361/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2956 - accuracy: 0.8809 - val_loss: 0.4817 - val_accuracy: 0.8247\n",
      "Epoch 362/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2951 - accuracy: 0.8824 - val_loss: 0.4811 - val_accuracy: 0.8247\n",
      "Epoch 363/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8798 - val_loss: 0.4818 - val_accuracy: 0.8262\n",
      "Epoch 364/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2957 - accuracy: 0.8802 - val_loss: 0.4832 - val_accuracy: 0.8292\n",
      "Epoch 365/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2956 - accuracy: 0.8791 - val_loss: 0.4814 - val_accuracy: 0.8292\n",
      "Epoch 366/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2946 - accuracy: 0.8813 - val_loss: 0.4832 - val_accuracy: 0.8321\n",
      "Epoch 367/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2953 - accuracy: 0.8791 - val_loss: 0.4831 - val_accuracy: 0.8262\n",
      "Epoch 368/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2949 - accuracy: 0.8809 - val_loss: 0.4823 - val_accuracy: 0.8262\n",
      "Epoch 369/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2952 - accuracy: 0.8806 - val_loss: 0.4819 - val_accuracy: 0.8292\n",
      "Epoch 370/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2947 - accuracy: 0.8802 - val_loss: 0.4829 - val_accuracy: 0.8306\n",
      "Epoch 371/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2945 - accuracy: 0.8820 - val_loss: 0.4842 - val_accuracy: 0.8247\n",
      "Epoch 372/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2947 - accuracy: 0.8820 - val_loss: 0.4821 - val_accuracy: 0.8218\n",
      "Epoch 373/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2947 - accuracy: 0.8839 - val_loss: 0.4861 - val_accuracy: 0.8262\n",
      "Epoch 374/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8817 - val_loss: 0.4829 - val_accuracy: 0.8247\n",
      "Epoch 375/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2941 - accuracy: 0.8813 - val_loss: 0.4834 - val_accuracy: 0.8277\n",
      "Epoch 376/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.8817 - val_loss: 0.4841 - val_accuracy: 0.8218\n",
      "Epoch 377/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2930 - accuracy: 0.8809 - val_loss: 0.4841 - val_accuracy: 0.8321\n",
      "Epoch 378/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2938 - accuracy: 0.8820 - val_loss: 0.4839 - val_accuracy: 0.8277\n",
      "Epoch 379/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2943 - accuracy: 0.8824 - val_loss: 0.4844 - val_accuracy: 0.8247\n",
      "Epoch 380/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2936 - accuracy: 0.8843 - val_loss: 0.4843 - val_accuracy: 0.8292\n",
      "Epoch 381/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2934 - accuracy: 0.8806 - val_loss: 0.4858 - val_accuracy: 0.8306\n",
      "Epoch 382/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2932 - accuracy: 0.8820 - val_loss: 0.4848 - val_accuracy: 0.8262\n",
      "Epoch 383/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2941 - accuracy: 0.8809 - val_loss: 0.4863 - val_accuracy: 0.8189\n",
      "Epoch 384/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2930 - accuracy: 0.8806 - val_loss: 0.4857 - val_accuracy: 0.8247\n",
      "Epoch 385/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2929 - accuracy: 0.8817 - val_loss: 0.4866 - val_accuracy: 0.8277\n",
      "Epoch 386/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2931 - accuracy: 0.8820 - val_loss: 0.4862 - val_accuracy: 0.8277\n",
      "Epoch 387/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2934 - accuracy: 0.8809 - val_loss: 0.4885 - val_accuracy: 0.8292\n",
      "Epoch 388/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2928 - accuracy: 0.8846 - val_loss: 0.4882 - val_accuracy: 0.8277\n",
      "Epoch 389/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2931 - accuracy: 0.8813 - val_loss: 0.4883 - val_accuracy: 0.8277\n",
      "Epoch 390/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2922 - accuracy: 0.8843 - val_loss: 0.4866 - val_accuracy: 0.8277\n",
      "Epoch 391/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2923 - accuracy: 0.8817 - val_loss: 0.4881 - val_accuracy: 0.8247\n",
      "Epoch 392/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2928 - accuracy: 0.8839 - val_loss: 0.4898 - val_accuracy: 0.8262\n",
      "Epoch 393/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2920 - accuracy: 0.8824 - val_loss: 0.4883 - val_accuracy: 0.8292\n",
      "Epoch 394/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2916 - accuracy: 0.8854 - val_loss: 0.4896 - val_accuracy: 0.8277\n",
      "Epoch 395/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2922 - accuracy: 0.8832 - val_loss: 0.4890 - val_accuracy: 0.8262\n",
      "Epoch 396/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.2921 - accuracy: 0.8846 - val_loss: 0.4910 - val_accuracy: 0.8262\n",
      "Epoch 397/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2917 - accuracy: 0.8839 - val_loss: 0.4902 - val_accuracy: 0.8247\n",
      "Epoch 398/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8850 - val_loss: 0.4904 - val_accuracy: 0.8203\n",
      "Epoch 399/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.8850 - val_loss: 0.4911 - val_accuracy: 0.8262\n",
      "Epoch 400/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2913 - accuracy: 0.8854 - val_loss: 0.4921 - val_accuracy: 0.8277\n",
      "Epoch 401/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.2911 - accuracy: 0.8843 - val_loss: 0.4921 - val_accuracy: 0.8233\n",
      "Epoch 402/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2915 - accuracy: 0.8835 - val_loss: 0.4909 - val_accuracy: 0.8262\n",
      "Epoch 403/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2915 - accuracy: 0.8824 - val_loss: 0.4940 - val_accuracy: 0.8277\n",
      "Epoch 404/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2916 - accuracy: 0.8835 - val_loss: 0.4927 - val_accuracy: 0.8247\n",
      "Epoch 405/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2909 - accuracy: 0.8832 - val_loss: 0.4918 - val_accuracy: 0.8218\n",
      "Epoch 406/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8861 - val_loss: 0.4926 - val_accuracy: 0.8218\n",
      "Epoch 407/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8861 - val_loss: 0.4954 - val_accuracy: 0.8277\n",
      "Epoch 408/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2909 - accuracy: 0.8846 - val_loss: 0.4925 - val_accuracy: 0.8218\n",
      "Epoch 409/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2902 - accuracy: 0.8861 - val_loss: 0.4953 - val_accuracy: 0.8262\n",
      "Epoch 410/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8824 - val_loss: 0.4938 - val_accuracy: 0.8218\n",
      "Epoch 411/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2907 - accuracy: 0.8865 - val_loss: 0.4928 - val_accuracy: 0.8262\n",
      "Epoch 412/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2908 - accuracy: 0.8865 - val_loss: 0.4942 - val_accuracy: 0.8233\n",
      "Epoch 413/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2899 - accuracy: 0.8839 - val_loss: 0.4931 - val_accuracy: 0.8277\n",
      "Epoch 414/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2900 - accuracy: 0.8839 - val_loss: 0.4927 - val_accuracy: 0.8247\n",
      "Epoch 415/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2897 - accuracy: 0.8854 - val_loss: 0.4955 - val_accuracy: 0.8292\n",
      "Epoch 416/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2915 - accuracy: 0.8843 - val_loss: 0.4943 - val_accuracy: 0.8277\n",
      "Epoch 417/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8846 - val_loss: 0.4938 - val_accuracy: 0.8247\n",
      "Epoch 418/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8817 - val_loss: 0.4936 - val_accuracy: 0.8189\n",
      "Epoch 419/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8850 - val_loss: 0.4942 - val_accuracy: 0.8218\n",
      "Epoch 420/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8839 - val_loss: 0.4941 - val_accuracy: 0.8233\n",
      "Epoch 421/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2898 - accuracy: 0.8865 - val_loss: 0.4949 - val_accuracy: 0.8203\n",
      "Epoch 422/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2895 - accuracy: 0.8839 - val_loss: 0.4953 - val_accuracy: 0.8277\n",
      "Epoch 423/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2901 - accuracy: 0.8846 - val_loss: 0.4950 - val_accuracy: 0.8233\n",
      "Epoch 424/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2898 - accuracy: 0.8854 - val_loss: 0.4948 - val_accuracy: 0.8203\n",
      "Epoch 425/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2893 - accuracy: 0.8843 - val_loss: 0.4968 - val_accuracy: 0.8247\n",
      "Epoch 426/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2893 - accuracy: 0.8846 - val_loss: 0.4970 - val_accuracy: 0.8233\n",
      "Epoch 427/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.8876 - val_loss: 0.4974 - val_accuracy: 0.8233\n",
      "Epoch 428/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2892 - accuracy: 0.8865 - val_loss: 0.4973 - val_accuracy: 0.8262\n",
      "Epoch 429/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8846 - val_loss: 0.4978 - val_accuracy: 0.8174\n",
      "Epoch 430/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2891 - accuracy: 0.8854 - val_loss: 0.4979 - val_accuracy: 0.8247\n",
      "Epoch 431/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8868 - val_loss: 0.4983 - val_accuracy: 0.8262\n",
      "Epoch 432/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2891 - accuracy: 0.8846 - val_loss: 0.4982 - val_accuracy: 0.8233\n",
      "Epoch 433/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2887 - accuracy: 0.8850 - val_loss: 0.4989 - val_accuracy: 0.8203\n",
      "Epoch 434/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2884 - accuracy: 0.8857 - val_loss: 0.5001 - val_accuracy: 0.8218\n",
      "Epoch 435/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2888 - accuracy: 0.8865 - val_loss: 0.4983 - val_accuracy: 0.8218\n",
      "Epoch 436/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8843 - val_loss: 0.4991 - val_accuracy: 0.8174\n",
      "Epoch 437/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8824 - val_loss: 0.4982 - val_accuracy: 0.8218\n",
      "Epoch 438/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2887 - accuracy: 0.8843 - val_loss: 0.5006 - val_accuracy: 0.8233\n",
      "Epoch 439/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2888 - accuracy: 0.8846 - val_loss: 0.4995 - val_accuracy: 0.8189\n",
      "Epoch 440/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2892 - accuracy: 0.8828 - val_loss: 0.4990 - val_accuracy: 0.8233\n",
      "Epoch 441/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.8865 - val_loss: 0.4999 - val_accuracy: 0.8262\n",
      "Epoch 442/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.2882 - accuracy: 0.8868 - val_loss: 0.4999 - val_accuracy: 0.8247\n",
      "Epoch 443/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2879 - accuracy: 0.8854 - val_loss: 0.5034 - val_accuracy: 0.8247\n",
      "Epoch 444/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2891 - accuracy: 0.8850 - val_loss: 0.5033 - val_accuracy: 0.8262\n",
      "Epoch 445/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2883 - accuracy: 0.8843 - val_loss: 0.5026 - val_accuracy: 0.8247\n",
      "Epoch 446/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2885 - accuracy: 0.8850 - val_loss: 0.5004 - val_accuracy: 0.8218\n",
      "Epoch 447/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2879 - accuracy: 0.8854 - val_loss: 0.5015 - val_accuracy: 0.8233\n",
      "Epoch 448/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2877 - accuracy: 0.8854 - val_loss: 0.5020 - val_accuracy: 0.8218\n",
      "Epoch 449/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2876 - accuracy: 0.8865 - val_loss: 0.5022 - val_accuracy: 0.8233\n",
      "Epoch 450/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.2880 - accuracy: 0.8876 - val_loss: 0.5018 - val_accuracy: 0.8189\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "\n",
    "hist=ann_clf.fit(X_train,y_train,batch_size = 57, epochs=450,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1884,
     "status": "ok",
     "timestamp": 1682440898513,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "N7kXnmjW83Wi",
    "outputId": "8f30b2e2-5312-4376-dd05-f50e2827f4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 1ms/step\n",
      "106/106 [==============================] - 0s 2ms/step\n",
      "TRAINING\n",
      "Accuracy Score: 0.8747051886792453\n",
      "Confusion Matrix: [[2791   80]\n",
      " [ 345  176]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      2871\n",
      "           1       0.69      0.34      0.45       521\n",
      "\n",
      "    accuracy                           0.87      3392\n",
      "   macro avg       0.79      0.65      0.69      3392\n",
      "weighted avg       0.86      0.87      0.86      3392\n",
      "\n",
      "TESTING\n",
      "Accuracy Score: 0.8301886792452831\n",
      "Confusion Matrix: [[692  33]\n",
      " [111  12]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.91       725\n",
      "           1       0.27      0.10      0.14       123\n",
      "\n",
      "    accuracy                           0.83       848\n",
      "   macro avg       0.56      0.53      0.52       848\n",
      "weighted avg       0.78      0.83      0.80       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred_ann = np.round(ann_clf.predict(X_test))\n",
    "y_ann_train_pred = np.round(ann_clf.predict(X_train))\n",
    "print(\"TRAINING\")\n",
    "print('Accuracy Score:', accuracy_score(y_train, y_ann_train_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_train, y_ann_train_pred))\n",
    "print('Classification Report:', classification_report(y_train, y_ann_train_pred))\n",
    "print(\"TESTING\")\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred_ann))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred_ann))\n",
    "print('Classification Report:', classification_report(y_test, y_pred_ann))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682440898514,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "vmw8mYULfRv_"
   },
   "outputs": [],
   "source": [
    "#print(type(y_test))\n",
    "#print(type(y_train))\n",
    "#y_train=y_train.values\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6532,
     "status": "ok",
     "timestamp": 1682440905038,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "7C4XGQbxWA18",
    "outputId": "21e897cd-23df-4d33-b61c-7fc0b30fc101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting boruta\n",
      "  Downloading Boruta-0.3-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.9/dist-packages (from boruta) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from boruta) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.9/dist-packages (from boruta) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->boruta) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.17.1->boruta) (1.2.0)\n",
      "Installing collected packages: boruta\n",
      "Successfully installed boruta-0.3\n"
     ]
    }
   ],
   "source": [
    "! pip install boruta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682440905039,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "dzY9kheQTxhG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgNEeodJcMkk"
   },
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682440905039,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "wqBXA1NewUWF"
   },
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a random forest classifier\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9488,
     "status": "ok",
     "timestamp": 1682440914520,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "lrdXLEbHwwWf",
    "outputId": "c49b69db-a00d-4faa-99fc-e8bb550ba0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t15\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t14 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t15 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t16 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t17 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t18 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t19 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t20 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t21 / 100\n",
      "Confirmed: \t7\n",
      "Tentative: \t1\n",
      "Rejected: \t7\n",
      "Iteration: \t22 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t0\n",
      "Rejected: \t7\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t23 / 100\n",
      "Confirmed: \t8\n",
      "Tentative: \t0\n",
      "Rejected: \t7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BorutaPy(estimator=RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=5,\n",
       "                                          n_estimators=80, n_jobs=-1,\n",
       "                                          random_state=RandomState(MT19937) at 0x7F69A04D1B40),\n",
       "         n_estimators=&#x27;auto&#x27;,\n",
       "         random_state=RandomState(MT19937) at 0x7F69A04D1B40, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BorutaPy</label><div class=\"sk-toggleable__content\"><pre>BorutaPy(estimator=RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=5,\n",
       "                                          n_estimators=80, n_jobs=-1,\n",
       "                                          random_state=RandomState(MT19937) at 0x7F69A04D1B40),\n",
       "         n_estimators=&#x27;auto&#x27;,\n",
       "         random_state=RandomState(MT19937) at 0x7F69A04D1B40, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=5, n_estimators=80,\n",
       "                       n_jobs=-1,\n",
       "                       random_state=RandomState(MT19937) at 0x7F69A04D1B40)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, max_depth=5, n_estimators=80,\n",
       "                       n_jobs=-1,\n",
       "                       random_state=RandomState(MT19937) at 0x7F69A04D1B40)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BorutaPy(estimator=RandomForestClassifier(class_weight='balanced', max_depth=5,\n",
       "                                          n_estimators=80, n_jobs=-1,\n",
       "                                          random_state=RandomState(MT19937) at 0x7F69A04D1B40),\n",
       "         n_estimators='auto',\n",
       "         random_state=RandomState(MT19937) at 0x7F69A04D1B40, verbose=2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boruta_feature_selector = BorutaPy(estimator=rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "\n",
    "# Fit the Boruta feature selector to the data\n",
    "boruta_feature_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682440914520,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "0O4dSLYDxzLW",
    "outputId": "f149bc1f-552a-4906-8964-9e43e210bd2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False,  True, False, False,  True, False,\n",
       "        True,  True,  True,  True, False,  True])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boruta_feature_selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682440914521,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "UE7SWdUMxz7t",
    "outputId": "4ce17906-5845-4e62-be15-8e33c6271eec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of significant features:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of significant features: \", boruta_feature_selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1682440915129,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "kSTKYQhvcFNs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1682440915130,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "83GShCKz0qLD"
   },
   "outputs": [],
   "source": [
    "# Transform the data to include only the selected features\n",
    "X_train = boruta_feature_selector.transform(X_train)\n",
    "X_test = boruta_feature_selector.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1682440915130,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "kgpDTY0r0qt_",
    "outputId": "c3915eab-fe21-44c5-c5b7-191fb654e325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3392, 8)\n",
      "(848, 8)\n",
      "(3392,)\n",
      "(848,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1682440915131,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "Rwg6PXj_cgdY",
    "outputId": "5e834c55-b010-4ea7-a19b-8471073d4ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FROM TRAINING DATASET :\n",
      "Accuracy score:  81.25\n",
      "Confusion matrix: \n",
      " [[2612  259]\n",
      " [ 377  144]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8739    0.9098    0.8915      2871\n",
      "           1     0.3573    0.2764    0.3117       521\n",
      "\n",
      "    accuracy                         0.8125      3392\n",
      "   macro avg     0.6156    0.5931    0.6016      3392\n",
      "weighted avg     0.7945    0.8125    0.8024      3392\n",
      "\n",
      "ACCURACY FROM TEST DATASET :\n",
      "Accuracy score:  82.19\n",
      "Confusion matrix: \n",
      " [[670  55]\n",
      " [ 96  27]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8747    0.9241    0.8987       725\n",
      "           1     0.3293    0.2195    0.2634       123\n",
      "\n",
      "    accuracy                         0.8219       848\n",
      "   macro avg     0.6020    0.5718    0.5811       848\n",
      "weighted avg     0.7956    0.8219    0.8066       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NaiveBayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Create Naive Bayes classifier object and train on the training data\n",
    "nbclf = GaussianNB()\n",
    "nbclf.fit(X_train, y_train)\n",
    "\n",
    "nb_train_pred = nbclf.predict(X_train)\n",
    "nb_y_pred = nbclf.predict(X_test)\n",
    "\n",
    "print(\"ACCURACY FROM TRAINING DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_train,nb_train_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_train,nb_train_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_train,nb_train_pred,digits=4))\n",
    "\n",
    "\n",
    "print(\"ACCURACY FROM TEST DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_test,nb_y_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_test,nb_y_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_test,nb_y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1682440915131,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "rhGb07nq_6ga"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1682440915132,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "CEjYXrN0cnOu",
    "outputId": "d8ea599c-dc90-4a30-c715-1a4354eedb5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FROM TRAINING DATASET :\n",
      "Accuracy score:  85.26\n",
      "Confusion matrix: \n",
      " [[2870    1]\n",
      " [ 499   22]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8519    0.9997    0.9199      2871\n",
      "           1     0.9565    0.0422    0.0809       521\n",
      "\n",
      "    accuracy                         0.8526      3392\n",
      "   macro avg     0.9042    0.5209    0.5004      3392\n",
      "weighted avg     0.8680    0.8526    0.7910      3392\n",
      "\n",
      "ACCURACY FROM TEST DATASET :\n",
      "Accuracy score:  85.5\n",
      "Confusion matrix: \n",
      " [[724   1]\n",
      " [122   1]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8558    0.9986    0.9217       725\n",
      "           1     0.5000    0.0081    0.0160       123\n",
      "\n",
      "    accuracy                         0.8550       848\n",
      "   macro avg     0.6779    0.5034    0.4689       848\n",
      "weighted avg     0.8042    0.8550    0.7903       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest \n",
    "\n",
    "\n",
    "\n",
    "# Create Random Forest classifier object and train on the training data\n",
    "rfclf = RandomForestClassifier(n_estimators=100, criterion='gini', random_state = 42,max_depth=6, min_samples_leaf=8)\n",
    "rfclf.fit(X_train, y_train)\n",
    "\n",
    "y_rfc_train_pred = rfclf.predict(X_train)\n",
    "rf_y_pred = rfclf.predict(X_test)\n",
    "\n",
    "print(\"ACCURACY FROM TRAINING DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_train,y_rfc_train_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_train,y_rfc_train_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_train,y_rfc_train_pred,digits=4))\n",
    "\n",
    "\n",
    "print(\"ACCURACY FROM TEST DATASET :\")\n",
    "print(\"Accuracy score: \",round(accuracy_score(y_test,rf_y_pred)*100,2))\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_test,rf_y_pred))\n",
    "print(\"Classification report: \\n\",classification_report(y_test,rf_y_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1682440915133,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "ZN7tEEjGQPer"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the model\n",
    "annclf = keras.Sequential([\n",
    "    keras.layers.Dense(16, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1682440915134,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "dHsgUDcUQPbW"
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "annclf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114094,
     "status": "ok",
     "timestamp": 1682441029212,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "tZzFWRDNQPYl",
    "outputId": "f2fe99d8-d5a6-4376-c536-65bfd885eaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6727 - val_loss: 0.5643 - val_accuracy: 0.8203\n",
      "Epoch 2/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.8352 - val_loss: 0.4671 - val_accuracy: 0.8454\n",
      "Epoch 3/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8463 - val_loss: 0.4176 - val_accuracy: 0.8513\n",
      "Epoch 4/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.4113 - accuracy: 0.8474 - val_loss: 0.4000 - val_accuracy: 0.8557\n",
      "Epoch 5/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8478 - val_loss: 0.3925 - val_accuracy: 0.8571\n",
      "Epoch 6/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8481 - val_loss: 0.3880 - val_accuracy: 0.8513\n",
      "Epoch 7/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3990 - accuracy: 0.8492 - val_loss: 0.3855 - val_accuracy: 0.8527\n",
      "Epoch 8/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3969 - accuracy: 0.8507 - val_loss: 0.3822 - val_accuracy: 0.8527\n",
      "Epoch 9/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8500 - val_loss: 0.3809 - val_accuracy: 0.8513\n",
      "Epoch 10/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8507 - val_loss: 0.3796 - val_accuracy: 0.8527\n",
      "Epoch 11/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3924 - accuracy: 0.8504 - val_loss: 0.3790 - val_accuracy: 0.8513\n",
      "Epoch 12/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.8507 - val_loss: 0.3780 - val_accuracy: 0.8513\n",
      "Epoch 13/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3907 - accuracy: 0.8507 - val_loss: 0.3765 - val_accuracy: 0.8542\n",
      "Epoch 14/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8507 - val_loss: 0.3754 - val_accuracy: 0.8557\n",
      "Epoch 15/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8511 - val_loss: 0.3748 - val_accuracy: 0.8542\n",
      "Epoch 16/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3883 - accuracy: 0.8504 - val_loss: 0.3738 - val_accuracy: 0.8513\n",
      "Epoch 17/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3876 - accuracy: 0.8526 - val_loss: 0.3737 - val_accuracy: 0.8527\n",
      "Epoch 18/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3869 - accuracy: 0.8511 - val_loss: 0.3730 - val_accuracy: 0.8557\n",
      "Epoch 19/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3864 - accuracy: 0.8515 - val_loss: 0.3733 - val_accuracy: 0.8542\n",
      "Epoch 20/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8515 - val_loss: 0.3732 - val_accuracy: 0.8542\n",
      "Epoch 21/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8518 - val_loss: 0.3725 - val_accuracy: 0.8557\n",
      "Epoch 22/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3846 - accuracy: 0.8526 - val_loss: 0.3709 - val_accuracy: 0.8557\n",
      "Epoch 23/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8522 - val_loss: 0.3724 - val_accuracy: 0.8571\n",
      "Epoch 24/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.8526 - val_loss: 0.3715 - val_accuracy: 0.8557\n",
      "Epoch 25/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8537 - val_loss: 0.3722 - val_accuracy: 0.8542\n",
      "Epoch 26/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3826 - accuracy: 0.8515 - val_loss: 0.3713 - val_accuracy: 0.8557\n",
      "Epoch 27/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3820 - accuracy: 0.8526 - val_loss: 0.3707 - val_accuracy: 0.8557\n",
      "Epoch 28/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3818 - accuracy: 0.8533 - val_loss: 0.3704 - val_accuracy: 0.8557\n",
      "Epoch 29/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8529 - val_loss: 0.3698 - val_accuracy: 0.8571\n",
      "Epoch 30/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3807 - accuracy: 0.8522 - val_loss: 0.3704 - val_accuracy: 0.8571\n",
      "Epoch 31/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3805 - accuracy: 0.8522 - val_loss: 0.3708 - val_accuracy: 0.8571\n",
      "Epoch 32/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8511 - val_loss: 0.3699 - val_accuracy: 0.8557\n",
      "Epoch 33/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8511 - val_loss: 0.3703 - val_accuracy: 0.8557\n",
      "Epoch 34/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.8515 - val_loss: 0.3708 - val_accuracy: 0.8571\n",
      "Epoch 35/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8526 - val_loss: 0.3715 - val_accuracy: 0.8586\n",
      "Epoch 36/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8511 - val_loss: 0.3710 - val_accuracy: 0.8586\n",
      "Epoch 37/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3775 - accuracy: 0.8518 - val_loss: 0.3719 - val_accuracy: 0.8527\n",
      "Epoch 38/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.8515 - val_loss: 0.3712 - val_accuracy: 0.8557\n",
      "Epoch 39/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3765 - accuracy: 0.8522 - val_loss: 0.3710 - val_accuracy: 0.8542\n",
      "Epoch 40/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.8511 - val_loss: 0.3712 - val_accuracy: 0.8586\n",
      "Epoch 41/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3761 - accuracy: 0.8529 - val_loss: 0.3715 - val_accuracy: 0.8616\n",
      "Epoch 42/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8515 - val_loss: 0.3717 - val_accuracy: 0.8542\n",
      "Epoch 43/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8515 - val_loss: 0.3713 - val_accuracy: 0.8586\n",
      "Epoch 44/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8522 - val_loss: 0.3715 - val_accuracy: 0.8542\n",
      "Epoch 45/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8518 - val_loss: 0.3718 - val_accuracy: 0.8557\n",
      "Epoch 46/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8515 - val_loss: 0.3731 - val_accuracy: 0.8527\n",
      "Epoch 47/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8526 - val_loss: 0.3729 - val_accuracy: 0.8557\n",
      "Epoch 48/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3738 - accuracy: 0.8511 - val_loss: 0.3724 - val_accuracy: 0.8557\n",
      "Epoch 49/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3741 - accuracy: 0.8522 - val_loss: 0.3730 - val_accuracy: 0.8557\n",
      "Epoch 50/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3731 - accuracy: 0.8518 - val_loss: 0.3727 - val_accuracy: 0.8527\n",
      "Epoch 51/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3730 - accuracy: 0.8518 - val_loss: 0.3726 - val_accuracy: 0.8586\n",
      "Epoch 52/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8526 - val_loss: 0.3732 - val_accuracy: 0.8557\n",
      "Epoch 53/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3722 - accuracy: 0.8518 - val_loss: 0.3728 - val_accuracy: 0.8571\n",
      "Epoch 54/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3718 - accuracy: 0.8511 - val_loss: 0.3730 - val_accuracy: 0.8571\n",
      "Epoch 55/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3718 - accuracy: 0.8526 - val_loss: 0.3738 - val_accuracy: 0.8557\n",
      "Epoch 56/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3712 - accuracy: 0.8518 - val_loss: 0.3740 - val_accuracy: 0.8571\n",
      "Epoch 57/450\n",
      "48/48 [==============================] - 1s 10ms/step - loss: 0.3710 - accuracy: 0.8529 - val_loss: 0.3745 - val_accuracy: 0.8527\n",
      "Epoch 58/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3710 - accuracy: 0.8511 - val_loss: 0.3752 - val_accuracy: 0.8527\n",
      "Epoch 59/450\n",
      "48/48 [==============================] - 1s 10ms/step - loss: 0.3706 - accuracy: 0.8526 - val_loss: 0.3740 - val_accuracy: 0.8557\n",
      "Epoch 60/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3706 - accuracy: 0.8515 - val_loss: 0.3753 - val_accuracy: 0.8527\n",
      "Epoch 61/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3701 - accuracy: 0.8522 - val_loss: 0.3749 - val_accuracy: 0.8542\n",
      "Epoch 62/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3697 - accuracy: 0.8518 - val_loss: 0.3745 - val_accuracy: 0.8527\n",
      "Epoch 63/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3697 - accuracy: 0.8522 - val_loss: 0.3745 - val_accuracy: 0.8527\n",
      "Epoch 64/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3688 - accuracy: 0.8522 - val_loss: 0.3747 - val_accuracy: 0.8557\n",
      "Epoch 65/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3690 - accuracy: 0.8529 - val_loss: 0.3748 - val_accuracy: 0.8557\n",
      "Epoch 66/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3698 - accuracy: 0.8533 - val_loss: 0.3758 - val_accuracy: 0.8542\n",
      "Epoch 67/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3688 - accuracy: 0.8533 - val_loss: 0.3746 - val_accuracy: 0.8527\n",
      "Epoch 68/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3685 - accuracy: 0.8529 - val_loss: 0.3749 - val_accuracy: 0.8542\n",
      "Epoch 69/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3677 - accuracy: 0.8551 - val_loss: 0.3743 - val_accuracy: 0.8557\n",
      "Epoch 70/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3679 - accuracy: 0.8526 - val_loss: 0.3752 - val_accuracy: 0.8527\n",
      "Epoch 71/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3677 - accuracy: 0.8533 - val_loss: 0.3761 - val_accuracy: 0.8513\n",
      "Epoch 72/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3672 - accuracy: 0.8540 - val_loss: 0.3760 - val_accuracy: 0.8542\n",
      "Epoch 73/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3671 - accuracy: 0.8551 - val_loss: 0.3760 - val_accuracy: 0.8542\n",
      "Epoch 74/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3669 - accuracy: 0.8537 - val_loss: 0.3752 - val_accuracy: 0.8527\n",
      "Epoch 75/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3663 - accuracy: 0.8537 - val_loss: 0.3765 - val_accuracy: 0.8513\n",
      "Epoch 76/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3661 - accuracy: 0.8540 - val_loss: 0.3763 - val_accuracy: 0.8527\n",
      "Epoch 77/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3660 - accuracy: 0.8529 - val_loss: 0.3763 - val_accuracy: 0.8498\n",
      "Epoch 78/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3661 - accuracy: 0.8540 - val_loss: 0.3775 - val_accuracy: 0.8513\n",
      "Epoch 79/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3659 - accuracy: 0.8555 - val_loss: 0.3762 - val_accuracy: 0.8513\n",
      "Epoch 80/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3649 - accuracy: 0.8555 - val_loss: 0.3773 - val_accuracy: 0.8513\n",
      "Epoch 81/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3653 - accuracy: 0.8548 - val_loss: 0.3773 - val_accuracy: 0.8513\n",
      "Epoch 82/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3647 - accuracy: 0.8551 - val_loss: 0.3772 - val_accuracy: 0.8498\n",
      "Epoch 83/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3651 - accuracy: 0.8551 - val_loss: 0.3775 - val_accuracy: 0.8498\n",
      "Epoch 84/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3648 - accuracy: 0.8555 - val_loss: 0.3782 - val_accuracy: 0.8513\n",
      "Epoch 85/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3644 - accuracy: 0.8544 - val_loss: 0.3789 - val_accuracy: 0.8513\n",
      "Epoch 86/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3642 - accuracy: 0.8566 - val_loss: 0.3794 - val_accuracy: 0.8468\n",
      "Epoch 87/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3644 - accuracy: 0.8574 - val_loss: 0.3777 - val_accuracy: 0.8498\n",
      "Epoch 88/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3637 - accuracy: 0.8551 - val_loss: 0.3791 - val_accuracy: 0.8468\n",
      "Epoch 89/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3638 - accuracy: 0.8562 - val_loss: 0.3796 - val_accuracy: 0.8468\n",
      "Epoch 90/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3632 - accuracy: 0.8559 - val_loss: 0.3797 - val_accuracy: 0.8454\n",
      "Epoch 91/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3630 - accuracy: 0.8562 - val_loss: 0.3798 - val_accuracy: 0.8439\n",
      "Epoch 92/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3627 - accuracy: 0.8570 - val_loss: 0.3792 - val_accuracy: 0.8439\n",
      "Epoch 93/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3626 - accuracy: 0.8581 - val_loss: 0.3798 - val_accuracy: 0.8454\n",
      "Epoch 94/450\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3633 - accuracy: 0.8581 - val_loss: 0.3800 - val_accuracy: 0.8468\n",
      "Epoch 95/450\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3627 - accuracy: 0.8577 - val_loss: 0.3796 - val_accuracy: 0.8468\n",
      "Epoch 96/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3616 - accuracy: 0.8581 - val_loss: 0.3816 - val_accuracy: 0.8468\n",
      "Epoch 97/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3620 - accuracy: 0.8581 - val_loss: 0.3818 - val_accuracy: 0.8439\n",
      "Epoch 98/450\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3624 - accuracy: 0.8581 - val_loss: 0.3816 - val_accuracy: 0.8439\n",
      "Epoch 99/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3616 - accuracy: 0.8574 - val_loss: 0.3815 - val_accuracy: 0.8409\n",
      "Epoch 100/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3614 - accuracy: 0.8592 - val_loss: 0.3823 - val_accuracy: 0.8424\n",
      "Epoch 101/450\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3617 - accuracy: 0.8574 - val_loss: 0.3827 - val_accuracy: 0.8439\n",
      "Epoch 102/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3612 - accuracy: 0.8585 - val_loss: 0.3822 - val_accuracy: 0.8454\n",
      "Epoch 103/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3608 - accuracy: 0.8588 - val_loss: 0.3823 - val_accuracy: 0.8439\n",
      "Epoch 104/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3613 - accuracy: 0.8577 - val_loss: 0.3838 - val_accuracy: 0.8439\n",
      "Epoch 105/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3609 - accuracy: 0.8581 - val_loss: 0.3828 - val_accuracy: 0.8468\n",
      "Epoch 106/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3608 - accuracy: 0.8588 - val_loss: 0.3834 - val_accuracy: 0.8424\n",
      "Epoch 107/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3609 - accuracy: 0.8588 - val_loss: 0.3846 - val_accuracy: 0.8468\n",
      "Epoch 108/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3606 - accuracy: 0.8599 - val_loss: 0.3839 - val_accuracy: 0.8439\n",
      "Epoch 109/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3601 - accuracy: 0.8596 - val_loss: 0.3841 - val_accuracy: 0.8424\n",
      "Epoch 110/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3600 - accuracy: 0.8596 - val_loss: 0.3834 - val_accuracy: 0.8439\n",
      "Epoch 111/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3594 - accuracy: 0.8585 - val_loss: 0.3841 - val_accuracy: 0.8439\n",
      "Epoch 112/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3595 - accuracy: 0.8592 - val_loss: 0.3852 - val_accuracy: 0.8454\n",
      "Epoch 113/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3594 - accuracy: 0.8625 - val_loss: 0.3854 - val_accuracy: 0.8424\n",
      "Epoch 114/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3594 - accuracy: 0.8592 - val_loss: 0.3852 - val_accuracy: 0.8439\n",
      "Epoch 115/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3593 - accuracy: 0.8603 - val_loss: 0.3847 - val_accuracy: 0.8454\n",
      "Epoch 116/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3586 - accuracy: 0.8599 - val_loss: 0.3847 - val_accuracy: 0.8439\n",
      "Epoch 117/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3587 - accuracy: 0.8610 - val_loss: 0.3845 - val_accuracy: 0.8454\n",
      "Epoch 118/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3590 - accuracy: 0.8596 - val_loss: 0.3860 - val_accuracy: 0.8409\n",
      "Epoch 119/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3587 - accuracy: 0.8614 - val_loss: 0.3845 - val_accuracy: 0.8468\n",
      "Epoch 120/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3583 - accuracy: 0.8599 - val_loss: 0.3855 - val_accuracy: 0.8439\n",
      "Epoch 121/450\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.3578 - accuracy: 0.8614 - val_loss: 0.3864 - val_accuracy: 0.8409\n",
      "Epoch 122/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3579 - accuracy: 0.8610 - val_loss: 0.3866 - val_accuracy: 0.8424\n",
      "Epoch 123/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3582 - accuracy: 0.8610 - val_loss: 0.3879 - val_accuracy: 0.8395\n",
      "Epoch 124/450\n",
      "48/48 [==============================] - 1s 16ms/step - loss: 0.3579 - accuracy: 0.8599 - val_loss: 0.3877 - val_accuracy: 0.8409\n",
      "Epoch 125/450\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.3582 - accuracy: 0.8621 - val_loss: 0.3879 - val_accuracy: 0.8409\n",
      "Epoch 126/450\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.3572 - accuracy: 0.8625 - val_loss: 0.3876 - val_accuracy: 0.8409\n",
      "Epoch 127/450\n",
      "48/48 [==============================] - 1s 14ms/step - loss: 0.3574 - accuracy: 0.8607 - val_loss: 0.3882 - val_accuracy: 0.8439\n",
      "Epoch 128/450\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.3572 - accuracy: 0.8607 - val_loss: 0.3877 - val_accuracy: 0.8424\n",
      "Epoch 129/450\n",
      "48/48 [==============================] - 1s 17ms/step - loss: 0.3574 - accuracy: 0.8592 - val_loss: 0.3879 - val_accuracy: 0.8409\n",
      "Epoch 130/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3570 - accuracy: 0.8610 - val_loss: 0.3880 - val_accuracy: 0.8424\n",
      "Epoch 131/450\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3569 - accuracy: 0.8607 - val_loss: 0.3882 - val_accuracy: 0.8395\n",
      "Epoch 132/450\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3568 - accuracy: 0.8614 - val_loss: 0.3894 - val_accuracy: 0.8468\n",
      "Epoch 133/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3563 - accuracy: 0.8607 - val_loss: 0.3883 - val_accuracy: 0.8424\n",
      "Epoch 134/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3566 - accuracy: 0.8629 - val_loss: 0.3877 - val_accuracy: 0.8454\n",
      "Epoch 135/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3557 - accuracy: 0.8633 - val_loss: 0.3898 - val_accuracy: 0.8468\n",
      "Epoch 136/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3568 - accuracy: 0.8610 - val_loss: 0.3902 - val_accuracy: 0.8409\n",
      "Epoch 137/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3565 - accuracy: 0.8610 - val_loss: 0.3899 - val_accuracy: 0.8424\n",
      "Epoch 138/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3560 - accuracy: 0.8603 - val_loss: 0.3898 - val_accuracy: 0.8468\n",
      "Epoch 139/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3557 - accuracy: 0.8607 - val_loss: 0.3901 - val_accuracy: 0.8365\n",
      "Epoch 140/450\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3555 - accuracy: 0.8621 - val_loss: 0.3897 - val_accuracy: 0.8439\n",
      "Epoch 141/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3570 - accuracy: 0.8614 - val_loss: 0.3900 - val_accuracy: 0.8424\n",
      "Epoch 142/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3568 - accuracy: 0.8592 - val_loss: 0.3899 - val_accuracy: 0.8409\n",
      "Epoch 143/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3563 - accuracy: 0.8596 - val_loss: 0.3909 - val_accuracy: 0.8424\n",
      "Epoch 144/450\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3551 - accuracy: 0.8621 - val_loss: 0.3904 - val_accuracy: 0.8409\n",
      "Epoch 145/450\n",
      "48/48 [==============================] - 1s 12ms/step - loss: 0.3548 - accuracy: 0.8614 - val_loss: 0.3937 - val_accuracy: 0.8395\n",
      "Epoch 146/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3547 - accuracy: 0.8625 - val_loss: 0.3915 - val_accuracy: 0.8439\n",
      "Epoch 147/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8618 - val_loss: 0.3920 - val_accuracy: 0.8424\n",
      "Epoch 148/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.8607 - val_loss: 0.3924 - val_accuracy: 0.8424\n",
      "Epoch 149/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8629 - val_loss: 0.3918 - val_accuracy: 0.8439\n",
      "Epoch 150/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8610 - val_loss: 0.3943 - val_accuracy: 0.8365\n",
      "Epoch 151/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8625 - val_loss: 0.3927 - val_accuracy: 0.8439\n",
      "Epoch 152/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3537 - accuracy: 0.8625 - val_loss: 0.3917 - val_accuracy: 0.8409\n",
      "Epoch 153/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8610 - val_loss: 0.3937 - val_accuracy: 0.8409\n",
      "Epoch 154/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8618 - val_loss: 0.3928 - val_accuracy: 0.8439\n",
      "Epoch 155/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3531 - accuracy: 0.8621 - val_loss: 0.3937 - val_accuracy: 0.8424\n",
      "Epoch 156/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8614 - val_loss: 0.3940 - val_accuracy: 0.8409\n",
      "Epoch 157/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8610 - val_loss: 0.3939 - val_accuracy: 0.8439\n",
      "Epoch 158/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8607 - val_loss: 0.3941 - val_accuracy: 0.8409\n",
      "Epoch 159/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8625 - val_loss: 0.3946 - val_accuracy: 0.8439\n",
      "Epoch 160/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8625 - val_loss: 0.3955 - val_accuracy: 0.8409\n",
      "Epoch 161/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8610 - val_loss: 0.3957 - val_accuracy: 0.8380\n",
      "Epoch 162/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8621 - val_loss: 0.3948 - val_accuracy: 0.8409\n",
      "Epoch 163/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8625 - val_loss: 0.3961 - val_accuracy: 0.8395\n",
      "Epoch 164/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3518 - accuracy: 0.8596 - val_loss: 0.3949 - val_accuracy: 0.8439\n",
      "Epoch 165/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8625 - val_loss: 0.3961 - val_accuracy: 0.8439\n",
      "Epoch 166/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3522 - accuracy: 0.8614 - val_loss: 0.3961 - val_accuracy: 0.8380\n",
      "Epoch 167/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3519 - accuracy: 0.8596 - val_loss: 0.3962 - val_accuracy: 0.8380\n",
      "Epoch 168/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3515 - accuracy: 0.8618 - val_loss: 0.3964 - val_accuracy: 0.8409\n",
      "Epoch 169/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8610 - val_loss: 0.3988 - val_accuracy: 0.8409\n",
      "Epoch 170/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8614 - val_loss: 0.3973 - val_accuracy: 0.8380\n",
      "Epoch 171/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8614 - val_loss: 0.3972 - val_accuracy: 0.8351\n",
      "Epoch 172/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3516 - accuracy: 0.8614 - val_loss: 0.3983 - val_accuracy: 0.8336\n",
      "Epoch 173/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3507 - accuracy: 0.8610 - val_loss: 0.3988 - val_accuracy: 0.8365\n",
      "Epoch 174/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3511 - accuracy: 0.8633 - val_loss: 0.3997 - val_accuracy: 0.8365\n",
      "Epoch 175/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3502 - accuracy: 0.8610 - val_loss: 0.3992 - val_accuracy: 0.8395\n",
      "Epoch 176/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3500 - accuracy: 0.8610 - val_loss: 0.3988 - val_accuracy: 0.8424\n",
      "Epoch 177/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3504 - accuracy: 0.8618 - val_loss: 0.4001 - val_accuracy: 0.8365\n",
      "Epoch 178/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3499 - accuracy: 0.8629 - val_loss: 0.3994 - val_accuracy: 0.8351\n",
      "Epoch 179/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3492 - accuracy: 0.8621 - val_loss: 0.3990 - val_accuracy: 0.8439\n",
      "Epoch 180/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3494 - accuracy: 0.8625 - val_loss: 0.4004 - val_accuracy: 0.8424\n",
      "Epoch 181/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3502 - accuracy: 0.8633 - val_loss: 0.3994 - val_accuracy: 0.8424\n",
      "Epoch 182/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3500 - accuracy: 0.8607 - val_loss: 0.4009 - val_accuracy: 0.8395\n",
      "Epoch 183/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3494 - accuracy: 0.8618 - val_loss: 0.3995 - val_accuracy: 0.8395\n",
      "Epoch 184/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3489 - accuracy: 0.8636 - val_loss: 0.4011 - val_accuracy: 0.8380\n",
      "Epoch 185/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3491 - accuracy: 0.8625 - val_loss: 0.4009 - val_accuracy: 0.8380\n",
      "Epoch 186/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.8621 - val_loss: 0.4042 - val_accuracy: 0.8336\n",
      "Epoch 187/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3485 - accuracy: 0.8636 - val_loss: 0.4017 - val_accuracy: 0.8395\n",
      "Epoch 188/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3480 - accuracy: 0.8614 - val_loss: 0.4027 - val_accuracy: 0.8409\n",
      "Epoch 189/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3485 - accuracy: 0.8640 - val_loss: 0.4042 - val_accuracy: 0.8380\n",
      "Epoch 190/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8625 - val_loss: 0.4024 - val_accuracy: 0.8365\n",
      "Epoch 191/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3489 - accuracy: 0.8629 - val_loss: 0.4033 - val_accuracy: 0.8380\n",
      "Epoch 192/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3477 - accuracy: 0.8621 - val_loss: 0.4035 - val_accuracy: 0.8409\n",
      "Epoch 193/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3479 - accuracy: 0.8629 - val_loss: 0.4033 - val_accuracy: 0.8395\n",
      "Epoch 194/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8636 - val_loss: 0.4032 - val_accuracy: 0.8351\n",
      "Epoch 195/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3471 - accuracy: 0.8621 - val_loss: 0.4016 - val_accuracy: 0.8424\n",
      "Epoch 196/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3474 - accuracy: 0.8640 - val_loss: 0.4039 - val_accuracy: 0.8395\n",
      "Epoch 197/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.8618 - val_loss: 0.4060 - val_accuracy: 0.8365\n",
      "Epoch 198/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3466 - accuracy: 0.8625 - val_loss: 0.4054 - val_accuracy: 0.8336\n",
      "Epoch 199/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8625 - val_loss: 0.4066 - val_accuracy: 0.8351\n",
      "Epoch 200/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.8640 - val_loss: 0.4046 - val_accuracy: 0.8365\n",
      "Epoch 201/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3465 - accuracy: 0.8629 - val_loss: 0.4050 - val_accuracy: 0.8336\n",
      "Epoch 202/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8647 - val_loss: 0.4058 - val_accuracy: 0.8380\n",
      "Epoch 203/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8625 - val_loss: 0.4061 - val_accuracy: 0.8380\n",
      "Epoch 204/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3461 - accuracy: 0.8618 - val_loss: 0.4069 - val_accuracy: 0.8306\n",
      "Epoch 205/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.8625 - val_loss: 0.4061 - val_accuracy: 0.8365\n",
      "Epoch 206/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8625 - val_loss: 0.4077 - val_accuracy: 0.8351\n",
      "Epoch 207/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3464 - accuracy: 0.8614 - val_loss: 0.4093 - val_accuracy: 0.8380\n",
      "Epoch 208/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.8618 - val_loss: 0.4088 - val_accuracy: 0.8351\n",
      "Epoch 209/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3459 - accuracy: 0.8640 - val_loss: 0.4064 - val_accuracy: 0.8351\n",
      "Epoch 210/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8618 - val_loss: 0.4081 - val_accuracy: 0.8336\n",
      "Epoch 211/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8629 - val_loss: 0.4067 - val_accuracy: 0.8351\n",
      "Epoch 212/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3448 - accuracy: 0.8636 - val_loss: 0.4087 - val_accuracy: 0.8365\n",
      "Epoch 213/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8625 - val_loss: 0.4081 - val_accuracy: 0.8292\n",
      "Epoch 214/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3458 - accuracy: 0.8647 - val_loss: 0.4095 - val_accuracy: 0.8306\n",
      "Epoch 215/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3451 - accuracy: 0.8644 - val_loss: 0.4089 - val_accuracy: 0.8351\n",
      "Epoch 216/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8651 - val_loss: 0.4098 - val_accuracy: 0.8336\n",
      "Epoch 217/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8647 - val_loss: 0.4099 - val_accuracy: 0.8336\n",
      "Epoch 218/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3444 - accuracy: 0.8633 - val_loss: 0.4108 - val_accuracy: 0.8336\n",
      "Epoch 219/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8636 - val_loss: 0.4103 - val_accuracy: 0.8306\n",
      "Epoch 220/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3442 - accuracy: 0.8658 - val_loss: 0.4113 - val_accuracy: 0.8336\n",
      "Epoch 221/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3443 - accuracy: 0.8647 - val_loss: 0.4091 - val_accuracy: 0.8321\n",
      "Epoch 222/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8633 - val_loss: 0.4108 - val_accuracy: 0.8292\n",
      "Epoch 223/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8629 - val_loss: 0.4102 - val_accuracy: 0.8321\n",
      "Epoch 224/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8625 - val_loss: 0.4129 - val_accuracy: 0.8292\n",
      "Epoch 225/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8640 - val_loss: 0.4110 - val_accuracy: 0.8292\n",
      "Epoch 226/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.8633 - val_loss: 0.4112 - val_accuracy: 0.8292\n",
      "Epoch 227/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8633 - val_loss: 0.4118 - val_accuracy: 0.8277\n",
      "Epoch 228/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8621 - val_loss: 0.4118 - val_accuracy: 0.8351\n",
      "Epoch 229/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8651 - val_loss: 0.4140 - val_accuracy: 0.8306\n",
      "Epoch 230/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3436 - accuracy: 0.8633 - val_loss: 0.4112 - val_accuracy: 0.8336\n",
      "Epoch 231/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3429 - accuracy: 0.8636 - val_loss: 0.4119 - val_accuracy: 0.8336\n",
      "Epoch 232/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.8651 - val_loss: 0.4123 - val_accuracy: 0.8336\n",
      "Epoch 233/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3428 - accuracy: 0.8647 - val_loss: 0.4125 - val_accuracy: 0.8321\n",
      "Epoch 234/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3434 - accuracy: 0.8640 - val_loss: 0.4151 - val_accuracy: 0.8306\n",
      "Epoch 235/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8647 - val_loss: 0.4132 - val_accuracy: 0.8351\n",
      "Epoch 236/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.8647 - val_loss: 0.4149 - val_accuracy: 0.8306\n",
      "Epoch 237/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3423 - accuracy: 0.8633 - val_loss: 0.4151 - val_accuracy: 0.8321\n",
      "Epoch 238/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8633 - val_loss: 0.4140 - val_accuracy: 0.8336\n",
      "Epoch 239/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8651 - val_loss: 0.4153 - val_accuracy: 0.8336\n",
      "Epoch 240/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8647 - val_loss: 0.4156 - val_accuracy: 0.8336\n",
      "Epoch 241/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3431 - accuracy: 0.8651 - val_loss: 0.4184 - val_accuracy: 0.8336\n",
      "Epoch 242/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8640 - val_loss: 0.4152 - val_accuracy: 0.8336\n",
      "Epoch 243/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8640 - val_loss: 0.4159 - val_accuracy: 0.8336\n",
      "Epoch 244/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3412 - accuracy: 0.8640 - val_loss: 0.4157 - val_accuracy: 0.8306\n",
      "Epoch 245/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.8644 - val_loss: 0.4180 - val_accuracy: 0.8306\n",
      "Epoch 246/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8655 - val_loss: 0.4155 - val_accuracy: 0.8321\n",
      "Epoch 247/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3417 - accuracy: 0.8636 - val_loss: 0.4181 - val_accuracy: 0.8321\n",
      "Epoch 248/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3423 - accuracy: 0.8647 - val_loss: 0.4184 - val_accuracy: 0.8306\n",
      "Epoch 249/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8629 - val_loss: 0.4166 - val_accuracy: 0.8336\n",
      "Epoch 250/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3413 - accuracy: 0.8633 - val_loss: 0.4186 - val_accuracy: 0.8336\n",
      "Epoch 251/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.8655 - val_loss: 0.4193 - val_accuracy: 0.8306\n",
      "Epoch 252/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3411 - accuracy: 0.8647 - val_loss: 0.4175 - val_accuracy: 0.8292\n",
      "Epoch 253/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.8636 - val_loss: 0.4193 - val_accuracy: 0.8321\n",
      "Epoch 254/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3415 - accuracy: 0.8651 - val_loss: 0.4186 - val_accuracy: 0.8336\n",
      "Epoch 255/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3405 - accuracy: 0.8644 - val_loss: 0.4186 - val_accuracy: 0.8306\n",
      "Epoch 256/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3402 - accuracy: 0.8644 - val_loss: 0.4219 - val_accuracy: 0.8321\n",
      "Epoch 257/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3402 - accuracy: 0.8651 - val_loss: 0.4202 - val_accuracy: 0.8321\n",
      "Epoch 258/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3399 - accuracy: 0.8640 - val_loss: 0.4207 - val_accuracy: 0.8321\n",
      "Epoch 259/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3402 - accuracy: 0.8633 - val_loss: 0.4222 - val_accuracy: 0.8321\n",
      "Epoch 260/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3412 - accuracy: 0.8636 - val_loss: 0.4200 - val_accuracy: 0.8321\n",
      "Epoch 261/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3403 - accuracy: 0.8629 - val_loss: 0.4197 - val_accuracy: 0.8306\n",
      "Epoch 262/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3397 - accuracy: 0.8644 - val_loss: 0.4207 - val_accuracy: 0.8321\n",
      "Epoch 263/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3398 - accuracy: 0.8636 - val_loss: 0.4208 - val_accuracy: 0.8336\n",
      "Epoch 264/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3395 - accuracy: 0.8655 - val_loss: 0.4216 - val_accuracy: 0.8321\n",
      "Epoch 265/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3395 - accuracy: 0.8669 - val_loss: 0.4222 - val_accuracy: 0.8306\n",
      "Epoch 266/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3415 - accuracy: 0.8647 - val_loss: 0.4230 - val_accuracy: 0.8292\n",
      "Epoch 267/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3399 - accuracy: 0.8633 - val_loss: 0.4232 - val_accuracy: 0.8351\n",
      "Epoch 268/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.8658 - val_loss: 0.4222 - val_accuracy: 0.8321\n",
      "Epoch 269/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3401 - accuracy: 0.8658 - val_loss: 0.4245 - val_accuracy: 0.8321\n",
      "Epoch 270/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3396 - accuracy: 0.8640 - val_loss: 0.4226 - val_accuracy: 0.8321\n",
      "Epoch 271/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3389 - accuracy: 0.8655 - val_loss: 0.4257 - val_accuracy: 0.8292\n",
      "Epoch 272/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3402 - accuracy: 0.8647 - val_loss: 0.4226 - val_accuracy: 0.8321\n",
      "Epoch 273/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8669 - val_loss: 0.4238 - val_accuracy: 0.8321\n",
      "Epoch 274/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8647 - val_loss: 0.4229 - val_accuracy: 0.8336\n",
      "Epoch 275/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.8629 - val_loss: 0.4263 - val_accuracy: 0.8365\n",
      "Epoch 276/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8666 - val_loss: 0.4258 - val_accuracy: 0.8351\n",
      "Epoch 277/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8669 - val_loss: 0.4249 - val_accuracy: 0.8351\n",
      "Epoch 278/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3383 - accuracy: 0.8644 - val_loss: 0.4258 - val_accuracy: 0.8351\n",
      "Epoch 279/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8651 - val_loss: 0.4271 - val_accuracy: 0.8336\n",
      "Epoch 280/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8636 - val_loss: 0.4260 - val_accuracy: 0.8336\n",
      "Epoch 281/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3381 - accuracy: 0.8662 - val_loss: 0.4263 - val_accuracy: 0.8351\n",
      "Epoch 282/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8651 - val_loss: 0.4263 - val_accuracy: 0.8351\n",
      "Epoch 283/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8658 - val_loss: 0.4255 - val_accuracy: 0.8351\n",
      "Epoch 284/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8640 - val_loss: 0.4276 - val_accuracy: 0.8321\n",
      "Epoch 285/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8647 - val_loss: 0.4274 - val_accuracy: 0.8321\n",
      "Epoch 286/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3384 - accuracy: 0.8673 - val_loss: 0.4258 - val_accuracy: 0.8306\n",
      "Epoch 287/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.8658 - val_loss: 0.4287 - val_accuracy: 0.8336\n",
      "Epoch 288/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8655 - val_loss: 0.4297 - val_accuracy: 0.8336\n",
      "Epoch 289/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8658 - val_loss: 0.4267 - val_accuracy: 0.8351\n",
      "Epoch 290/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8644 - val_loss: 0.4277 - val_accuracy: 0.8351\n",
      "Epoch 291/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8658 - val_loss: 0.4269 - val_accuracy: 0.8336\n",
      "Epoch 292/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3378 - accuracy: 0.8666 - val_loss: 0.4269 - val_accuracy: 0.8365\n",
      "Epoch 293/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8655 - val_loss: 0.4263 - val_accuracy: 0.8321\n",
      "Epoch 294/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8651 - val_loss: 0.4271 - val_accuracy: 0.8351\n",
      "Epoch 295/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8662 - val_loss: 0.4273 - val_accuracy: 0.8351\n",
      "Epoch 296/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8662 - val_loss: 0.4265 - val_accuracy: 0.8306\n",
      "Epoch 297/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8633 - val_loss: 0.4286 - val_accuracy: 0.8306\n",
      "Epoch 298/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8658 - val_loss: 0.4288 - val_accuracy: 0.8306\n",
      "Epoch 299/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8655 - val_loss: 0.4303 - val_accuracy: 0.8321\n",
      "Epoch 300/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8651 - val_loss: 0.4297 - val_accuracy: 0.8277\n",
      "Epoch 301/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8669 - val_loss: 0.4307 - val_accuracy: 0.8262\n",
      "Epoch 302/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3369 - accuracy: 0.8647 - val_loss: 0.4320 - val_accuracy: 0.8292\n",
      "Epoch 303/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8655 - val_loss: 0.4282 - val_accuracy: 0.8306\n",
      "Epoch 304/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3365 - accuracy: 0.8658 - val_loss: 0.4281 - val_accuracy: 0.8306\n",
      "Epoch 305/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8662 - val_loss: 0.4299 - val_accuracy: 0.8336\n",
      "Epoch 306/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8640 - val_loss: 0.4321 - val_accuracy: 0.8306\n",
      "Epoch 307/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8677 - val_loss: 0.4306 - val_accuracy: 0.8321\n",
      "Epoch 308/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3366 - accuracy: 0.8669 - val_loss: 0.4306 - val_accuracy: 0.8292\n",
      "Epoch 309/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8658 - val_loss: 0.4280 - val_accuracy: 0.8351\n",
      "Epoch 310/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3366 - accuracy: 0.8658 - val_loss: 0.4304 - val_accuracy: 0.8336\n",
      "Epoch 311/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3358 - accuracy: 0.8662 - val_loss: 0.4306 - val_accuracy: 0.8292\n",
      "Epoch 312/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8677 - val_loss: 0.4316 - val_accuracy: 0.8336\n",
      "Epoch 313/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8673 - val_loss: 0.4303 - val_accuracy: 0.8336\n",
      "Epoch 314/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8673 - val_loss: 0.4328 - val_accuracy: 0.8292\n",
      "Epoch 315/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8636 - val_loss: 0.4339 - val_accuracy: 0.8262\n",
      "Epoch 316/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3365 - accuracy: 0.8655 - val_loss: 0.4337 - val_accuracy: 0.8292\n",
      "Epoch 317/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.8647 - val_loss: 0.4338 - val_accuracy: 0.8306\n",
      "Epoch 318/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8636 - val_loss: 0.4323 - val_accuracy: 0.8321\n",
      "Epoch 319/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8658 - val_loss: 0.4339 - val_accuracy: 0.8292\n",
      "Epoch 320/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3356 - accuracy: 0.8673 - val_loss: 0.4337 - val_accuracy: 0.8336\n",
      "Epoch 321/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.8680 - val_loss: 0.4319 - val_accuracy: 0.8306\n",
      "Epoch 322/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3369 - accuracy: 0.8662 - val_loss: 0.4332 - val_accuracy: 0.8336\n",
      "Epoch 323/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8677 - val_loss: 0.4353 - val_accuracy: 0.8306\n",
      "Epoch 324/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3355 - accuracy: 0.8644 - val_loss: 0.4356 - val_accuracy: 0.8306\n",
      "Epoch 325/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3358 - accuracy: 0.8662 - val_loss: 0.4339 - val_accuracy: 0.8336\n",
      "Epoch 326/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3361 - accuracy: 0.8662 - val_loss: 0.4351 - val_accuracy: 0.8321\n",
      "Epoch 327/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3355 - accuracy: 0.8673 - val_loss: 0.4344 - val_accuracy: 0.8292\n",
      "Epoch 328/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.8673 - val_loss: 0.4358 - val_accuracy: 0.8321\n",
      "Epoch 329/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8669 - val_loss: 0.4339 - val_accuracy: 0.8321\n",
      "Epoch 330/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3354 - accuracy: 0.8662 - val_loss: 0.4367 - val_accuracy: 0.8262\n",
      "Epoch 331/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.8655 - val_loss: 0.4374 - val_accuracy: 0.8277\n",
      "Epoch 332/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8662 - val_loss: 0.4364 - val_accuracy: 0.8262\n",
      "Epoch 333/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3353 - accuracy: 0.8680 - val_loss: 0.4359 - val_accuracy: 0.8292\n",
      "Epoch 334/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3352 - accuracy: 0.8666 - val_loss: 0.4353 - val_accuracy: 0.8321\n",
      "Epoch 335/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3350 - accuracy: 0.8666 - val_loss: 0.4362 - val_accuracy: 0.8292\n",
      "Epoch 336/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3357 - accuracy: 0.8669 - val_loss: 0.4378 - val_accuracy: 0.8321\n",
      "Epoch 337/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8658 - val_loss: 0.4353 - val_accuracy: 0.8277\n",
      "Epoch 338/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3348 - accuracy: 0.8684 - val_loss: 0.4405 - val_accuracy: 0.8336\n",
      "Epoch 339/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3350 - accuracy: 0.8662 - val_loss: 0.4348 - val_accuracy: 0.8277\n",
      "Epoch 340/450\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3349 - accuracy: 0.8673 - val_loss: 0.4369 - val_accuracy: 0.8306\n",
      "Epoch 341/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3343 - accuracy: 0.8680 - val_loss: 0.4363 - val_accuracy: 0.8321\n",
      "Epoch 342/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3345 - accuracy: 0.8666 - val_loss: 0.4376 - val_accuracy: 0.8306\n",
      "Epoch 343/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8666 - val_loss: 0.4353 - val_accuracy: 0.8306\n",
      "Epoch 344/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3343 - accuracy: 0.8651 - val_loss: 0.4366 - val_accuracy: 0.8336\n",
      "Epoch 345/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3343 - accuracy: 0.8669 - val_loss: 0.4355 - val_accuracy: 0.8306\n",
      "Epoch 346/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3361 - accuracy: 0.8651 - val_loss: 0.4397 - val_accuracy: 0.8306\n",
      "Epoch 347/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.8688 - val_loss: 0.4359 - val_accuracy: 0.8277\n",
      "Epoch 348/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.8669 - val_loss: 0.4380 - val_accuracy: 0.8321\n",
      "Epoch 349/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8680 - val_loss: 0.4378 - val_accuracy: 0.8321\n",
      "Epoch 350/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3349 - accuracy: 0.8669 - val_loss: 0.4377 - val_accuracy: 0.8292\n",
      "Epoch 351/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3339 - accuracy: 0.8662 - val_loss: 0.4386 - val_accuracy: 0.8292\n",
      "Epoch 352/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.8658 - val_loss: 0.4401 - val_accuracy: 0.8277\n",
      "Epoch 353/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.8658 - val_loss: 0.4414 - val_accuracy: 0.8292\n",
      "Epoch 354/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3343 - accuracy: 0.8684 - val_loss: 0.4407 - val_accuracy: 0.8321\n",
      "Epoch 355/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3349 - accuracy: 0.8658 - val_loss: 0.4389 - val_accuracy: 0.8306\n",
      "Epoch 356/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3338 - accuracy: 0.8666 - val_loss: 0.4369 - val_accuracy: 0.8321\n",
      "Epoch 357/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.8655 - val_loss: 0.4386 - val_accuracy: 0.8321\n",
      "Epoch 358/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8655 - val_loss: 0.4415 - val_accuracy: 0.8336\n",
      "Epoch 359/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8651 - val_loss: 0.4406 - val_accuracy: 0.8262\n",
      "Epoch 360/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8666 - val_loss: 0.4415 - val_accuracy: 0.8321\n",
      "Epoch 361/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8658 - val_loss: 0.4422 - val_accuracy: 0.8292\n",
      "Epoch 362/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8666 - val_loss: 0.4419 - val_accuracy: 0.8336\n",
      "Epoch 363/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8673 - val_loss: 0.4417 - val_accuracy: 0.8336\n",
      "Epoch 364/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3337 - accuracy: 0.8658 - val_loss: 0.4430 - val_accuracy: 0.8336\n",
      "Epoch 365/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.8673 - val_loss: 0.4435 - val_accuracy: 0.8306\n",
      "Epoch 366/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8647 - val_loss: 0.4418 - val_accuracy: 0.8292\n",
      "Epoch 367/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8669 - val_loss: 0.4419 - val_accuracy: 0.8262\n",
      "Epoch 368/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.8688 - val_loss: 0.4424 - val_accuracy: 0.8292\n",
      "Epoch 369/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.8673 - val_loss: 0.4420 - val_accuracy: 0.8277\n",
      "Epoch 370/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8658 - val_loss: 0.4446 - val_accuracy: 0.8306\n",
      "Epoch 371/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8684 - val_loss: 0.4421 - val_accuracy: 0.8277\n",
      "Epoch 372/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3334 - accuracy: 0.8666 - val_loss: 0.4428 - val_accuracy: 0.8336\n",
      "Epoch 373/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8647 - val_loss: 0.4436 - val_accuracy: 0.8321\n",
      "Epoch 374/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8666 - val_loss: 0.4444 - val_accuracy: 0.8277\n",
      "Epoch 375/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8662 - val_loss: 0.4452 - val_accuracy: 0.8292\n",
      "Epoch 376/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8680 - val_loss: 0.4429 - val_accuracy: 0.8306\n",
      "Epoch 377/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3333 - accuracy: 0.8662 - val_loss: 0.4431 - val_accuracy: 0.8277\n",
      "Epoch 378/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3319 - accuracy: 0.8677 - val_loss: 0.4429 - val_accuracy: 0.8336\n",
      "Epoch 379/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3321 - accuracy: 0.8651 - val_loss: 0.4443 - val_accuracy: 0.8277\n",
      "Epoch 380/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3321 - accuracy: 0.8651 - val_loss: 0.4451 - val_accuracy: 0.8336\n",
      "Epoch 381/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8677 - val_loss: 0.4441 - val_accuracy: 0.8336\n",
      "Epoch 382/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8680 - val_loss: 0.4438 - val_accuracy: 0.8277\n",
      "Epoch 383/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3324 - accuracy: 0.8662 - val_loss: 0.4434 - val_accuracy: 0.8292\n",
      "Epoch 384/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3311 - accuracy: 0.8673 - val_loss: 0.4461 - val_accuracy: 0.8321\n",
      "Epoch 385/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.8633 - val_loss: 0.4441 - val_accuracy: 0.8277\n",
      "Epoch 386/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3328 - accuracy: 0.8658 - val_loss: 0.4446 - val_accuracy: 0.8277\n",
      "Epoch 387/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3313 - accuracy: 0.8673 - val_loss: 0.4453 - val_accuracy: 0.8306\n",
      "Epoch 388/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.8669 - val_loss: 0.4454 - val_accuracy: 0.8336\n",
      "Epoch 389/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3320 - accuracy: 0.8647 - val_loss: 0.4461 - val_accuracy: 0.8277\n",
      "Epoch 390/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.8680 - val_loss: 0.4449 - val_accuracy: 0.8277\n",
      "Epoch 391/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3316 - accuracy: 0.8684 - val_loss: 0.4437 - val_accuracy: 0.8277\n",
      "Epoch 392/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8673 - val_loss: 0.4483 - val_accuracy: 0.8277\n",
      "Epoch 393/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3322 - accuracy: 0.8658 - val_loss: 0.4465 - val_accuracy: 0.8321\n",
      "Epoch 394/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.8669 - val_loss: 0.4462 - val_accuracy: 0.8277\n",
      "Epoch 395/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8669 - val_loss: 0.4482 - val_accuracy: 0.8247\n",
      "Epoch 396/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3309 - accuracy: 0.8655 - val_loss: 0.4455 - val_accuracy: 0.8277\n",
      "Epoch 397/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3313 - accuracy: 0.8662 - val_loss: 0.4474 - val_accuracy: 0.8262\n",
      "Epoch 398/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3308 - accuracy: 0.8655 - val_loss: 0.4466 - val_accuracy: 0.8292\n",
      "Epoch 399/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3322 - accuracy: 0.8651 - val_loss: 0.4462 - val_accuracy: 0.8262\n",
      "Epoch 400/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8666 - val_loss: 0.4467 - val_accuracy: 0.8292\n",
      "Epoch 401/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3308 - accuracy: 0.8658 - val_loss: 0.4486 - val_accuracy: 0.8277\n",
      "Epoch 402/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8673 - val_loss: 0.4474 - val_accuracy: 0.8277\n",
      "Epoch 403/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3310 - accuracy: 0.8673 - val_loss: 0.4481 - val_accuracy: 0.8277\n",
      "Epoch 404/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.8677 - val_loss: 0.4506 - val_accuracy: 0.8262\n",
      "Epoch 405/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8673 - val_loss: 0.4493 - val_accuracy: 0.8306\n",
      "Epoch 406/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3313 - accuracy: 0.8688 - val_loss: 0.4472 - val_accuracy: 0.8306\n",
      "Epoch 407/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8673 - val_loss: 0.4501 - val_accuracy: 0.8292\n",
      "Epoch 408/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3311 - accuracy: 0.8658 - val_loss: 0.4501 - val_accuracy: 0.8277\n",
      "Epoch 409/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3300 - accuracy: 0.8680 - val_loss: 0.4511 - val_accuracy: 0.8277\n",
      "Epoch 410/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.8666 - val_loss: 0.4487 - val_accuracy: 0.8336\n",
      "Epoch 411/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3308 - accuracy: 0.8691 - val_loss: 0.4492 - val_accuracy: 0.8277\n",
      "Epoch 412/450\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.8662 - val_loss: 0.4496 - val_accuracy: 0.8277\n",
      "Epoch 413/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8666 - val_loss: 0.4501 - val_accuracy: 0.8277\n",
      "Epoch 414/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8655 - val_loss: 0.4465 - val_accuracy: 0.8292\n",
      "Epoch 415/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8673 - val_loss: 0.4495 - val_accuracy: 0.8277\n",
      "Epoch 416/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8688 - val_loss: 0.4511 - val_accuracy: 0.8277\n",
      "Epoch 417/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3296 - accuracy: 0.8677 - val_loss: 0.4491 - val_accuracy: 0.8277\n",
      "Epoch 418/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.8655 - val_loss: 0.4504 - val_accuracy: 0.8262\n",
      "Epoch 419/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3292 - accuracy: 0.8680 - val_loss: 0.4521 - val_accuracy: 0.8306\n",
      "Epoch 420/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8669 - val_loss: 0.4482 - val_accuracy: 0.8292\n",
      "Epoch 421/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8666 - val_loss: 0.4493 - val_accuracy: 0.8292\n",
      "Epoch 422/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3308 - accuracy: 0.8647 - val_loss: 0.4506 - val_accuracy: 0.8306\n",
      "Epoch 423/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3298 - accuracy: 0.8699 - val_loss: 0.4511 - val_accuracy: 0.8277\n",
      "Epoch 424/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8673 - val_loss: 0.4499 - val_accuracy: 0.8292\n",
      "Epoch 425/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3296 - accuracy: 0.8655 - val_loss: 0.4521 - val_accuracy: 0.8247\n",
      "Epoch 426/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8688 - val_loss: 0.4502 - val_accuracy: 0.8292\n",
      "Epoch 427/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3299 - accuracy: 0.8655 - val_loss: 0.4511 - val_accuracy: 0.8277\n",
      "Epoch 428/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8677 - val_loss: 0.4491 - val_accuracy: 0.8306\n",
      "Epoch 429/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.8673 - val_loss: 0.4508 - val_accuracy: 0.8277\n",
      "Epoch 430/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.8688 - val_loss: 0.4500 - val_accuracy: 0.8292\n",
      "Epoch 431/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8666 - val_loss: 0.4502 - val_accuracy: 0.8292\n",
      "Epoch 432/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.8680 - val_loss: 0.4499 - val_accuracy: 0.8277\n",
      "Epoch 433/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3304 - accuracy: 0.8666 - val_loss: 0.4540 - val_accuracy: 0.8233\n",
      "Epoch 434/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.8684 - val_loss: 0.4505 - val_accuracy: 0.8292\n",
      "Epoch 435/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8662 - val_loss: 0.4522 - val_accuracy: 0.8277\n",
      "Epoch 436/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8669 - val_loss: 0.4521 - val_accuracy: 0.8262\n",
      "Epoch 437/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8680 - val_loss: 0.4526 - val_accuracy: 0.8262\n",
      "Epoch 438/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.8666 - val_loss: 0.4494 - val_accuracy: 0.8292\n",
      "Epoch 439/450\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8669 - val_loss: 0.4540 - val_accuracy: 0.8277\n",
      "Epoch 440/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3279 - accuracy: 0.8680 - val_loss: 0.4538 - val_accuracy: 0.8321\n",
      "Epoch 441/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8703 - val_loss: 0.4524 - val_accuracy: 0.8277\n",
      "Epoch 442/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3282 - accuracy: 0.8680 - val_loss: 0.4509 - val_accuracy: 0.8292\n",
      "Epoch 443/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.8680 - val_loss: 0.4529 - val_accuracy: 0.8277\n",
      "Epoch 444/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3286 - accuracy: 0.8669 - val_loss: 0.4530 - val_accuracy: 0.8262\n",
      "Epoch 445/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3287 - accuracy: 0.8680 - val_loss: 0.4521 - val_accuracy: 0.8292\n",
      "Epoch 446/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.8680 - val_loss: 0.4531 - val_accuracy: 0.8277\n",
      "Epoch 447/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.8673 - val_loss: 0.4510 - val_accuracy: 0.8321\n",
      "Epoch 448/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.8666 - val_loss: 0.4538 - val_accuracy: 0.8247\n",
      "Epoch 449/450\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.3279 - accuracy: 0.8699 - val_loss: 0.4533 - val_accuracy: 0.8277\n",
      "Epoch 450/450\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.3287 - accuracy: 0.8673 - val_loss: 0.4517 - val_accuracy: 0.8292\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "\n",
    "\n",
    "hist=annclf.fit(X_train,y_train,batch_size = 57, epochs=450,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1682441029212,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "Q53Pozc7QPV3",
    "outputId": "7854a727-d8d0-4e39-80b7-7c9661d06343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 1ms/step\n",
      "106/106 [==============================] - 0s 1ms/step\n",
      "TRAINING\n",
      "Accuracy Score: 0.8608490566037735\n",
      "Confusion Matrix: [[2815   56]\n",
      " [ 416  105]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      2871\n",
      "           1       0.65      0.20      0.31       521\n",
      "\n",
      "    accuracy                           0.86      3392\n",
      "   macro avg       0.76      0.59      0.62      3392\n",
      "weighted avg       0.84      0.86      0.83      3392\n",
      "\n",
      "TESTING\n",
      "Accuracy Score: 0.8384433962264151\n",
      "Confusion Matrix: [[701  24]\n",
      " [113  10]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91       725\n",
      "           1       0.29      0.08      0.13       123\n",
      "\n",
      "    accuracy                           0.84       848\n",
      "   macro avg       0.58      0.52      0.52       848\n",
      "weighted avg       0.78      0.84      0.80       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_pred_ann = np.round(annclf.predict(X_test))\n",
    "y_ann_train_pred = np.round(annclf.predict(X_train))\n",
    "print(\"TRAINING\")\n",
    "print('Accuracy Score:', accuracy_score(y_train, y_ann_train_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_train, y_ann_train_pred))\n",
    "print('Classification Report:', classification_report(y_train, y_ann_train_pred))\n",
    "print(\"TESTING\")\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred_ann))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred_ann))\n",
    "print('Classification Report:', classification_report(y_test, y_pred_ann))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1682441029213,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "F3XE_e58QPN2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1682441029213,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "yA9NkwJbqzmO"
   },
   "outputs": [],
   "source": [
    "n=[[1,39,4,0,0,0,0,0,0,195,106,70,26.97,80,77],\n",
    "   [0,46,2,0,0,0,0,0,0,250,121,81,28.73,95,76],\n",
    "   [1,48,1,1,20,0,0,0,0,245,127.5,80,25.34,75,70],\n",
    "   [0,61,3,1,30,0,0,1,0,225,150,95,28.58,65,103],\n",
    "   [0,46,3,1,23,0,0,0,0,285,130,84,23.1,85,85],\n",
    "   [0,43,2,0,0,0,0,1,0,228,180,110,30.3,77,99],\n",
    "   [0,63,1,0,0,0,0,0,0,205,138,71,33.11,60,85],\n",
    "   [0,45,2,1,20,0,0,0,0,313,100,71,21.68,79,78],\n",
    "   [1,52,1,0,0,0,0,1,0,260,141.5,89,26.36,76,79],\n",
    "   [1,43,1,1,30,0,0,1,0,225,162,107,23.61,93,88],\n",
    "   [0,46,1,1,10,0,0,0,0,250,116,71,20.35,88,94],\n",
    "   [0,54,1,1,9,0,0,0,1,266,114,76,17.61,88,55],\n",
    "   [1,49,1,1,2,0,0,1,0,255,143.5,81,25.65,75,80],\n",
    "   [1,44,2,0,0,0,0,0,0,185,115,69,22.29,65,82],\n",
    "   [0,40,4,1,20,0,0,0,0,205,158,102,25.45,75,87],\n",
    "   [1,56,4,1,20,0,0,0,0,270,121,79,23.58,95,93],\n",
    "   [0,67,1,0,0,0,0,1,0,254,157,89,24.25,60,74],\n",
    "   [1,53,1,1,20,0,0,0,0,220,123.5,75,19.64,78,73],\n",
    "   [0,57,1,1,3,0,0,0,0,235,126.5,80,24.88,83,72],\n",
    "   [1,57,1,0,0,0,0,0,0,220,136,84,26.84,75,64],\n",
    "   [0,63,1,0,0,0,0,1,0,252,154,87,28.6,72,45],\n",
    "   [0,62,1,0,0,0,0,1,1,212,190,99,29.64,100,202],\n",
    "   [1,38,1,1,20,0,0,0,0,223,107,73,23.01,85,78],\n",
    "   [0,47,2,1,20,0,0,0,0,300,112.5,60,20.13,76,83],\n",
    "   [0,52,4,0,0,0,0,0,0,302,110,67.5,23.51,63,87],\n",
    "   [0,45,1,0,0,0,0,1,0,183,151,101,45.8,80,63],\n",
    "   [1,41,2,1,43,0,0,1,0,274,152,90,30.58,85,65],\n",
    "   [1,60,1,1,20,0,0,1,0,170,179,94,26.52,90,83],\n",
    "   [1,39,2,0,0,0,0,1,0,285,155,110,32.51,84,70],\n",
    "   [0,53,2,0,0,0,0,0,0,210,138,86.5,22.49,88,87],\n",
    "   [0,52,1,1,15,0,0,0,0,170,124,78,26.03,75,82],\n",
    "   [0,61,1,0,0,0,0,1,0,110,182,101,29.35,70,83],\n",
    "   [0,36,2,1,15,0,0,0,0,197,113,72.5,22.73,70,65],\n",
    "   [0,62,1,0,0,0,0,0,0,261,138,82,23.89,65,77],\n",
    "   [0,61,2,1,1,0,0,1,0,326,200,104,38.46,57,78],\n",
    "   [1,41,4,1,43,0,0,0,0,252,124,86,28.56,100,70],\n",
    "   [1,41,2,0,0,0,0,0,0,274,121,61.5,25.42,80,76],\n",
    "   [1,53,1,1,20,0,0,0,0,188,138,89,18.23,60,75],\n",
    "   [1,39,2,1,15,0,0,0,0,256,132.5,80,24.8,75,97],\n",
    "   [0,51,1,0,0,0,0,0,0,244,102,71.5,27.38,71,77],\n",
    "   [0,66,1,0,0,0,0,1,0,311,154,80,28.55,60,104],\n",
    "   [1,60,4,1,30,0,0,0,0,243,126,79,28.57,80,65],\n",
    "   [0,65,3,0,0,0,0,0,0,193,123,76.5,29.33,60,96],\n",
    "   [0,63,4,1,20,0,0,0,1,239,134,80,26.64,88,126],\n",
    "   [0,56,1,0,0,0,0,1,0,296,180,90,23.72,75,120],\n",
    "   [0,56,1,1,15,0,0,0,0,269,121,75,22.36,50,66],\n",
    "   [0,47,2,1,20,0,0,0,0,220,132.5,87,27.98,85,75],\n",
    "   [0,60,3,0,0,0,0,1,0,275,141,84,29.66,75,105],\n",
    "   [0,45,3,1,9,0,0,0,0,268,110,64,20.68,63,71],\n",
    "   [0,48,1,0,0,0,0,1,0,265,145,77,24.23,74,64],\n",
    "   [0,46,1,1,20,0,0,0,0,173,100,63,23.25,65,99],\n",
    "   [0,63,1,0,0,0,0,0,0,273,135,82,26.76,85,56],\n",
    "   [0,42,4,0,0,0,0,0,0,250,115,79,26.93,65,79],\n",
    "   [1,40,1,1,43,0,0,1,0,290,138,90,27.54,85,73]\n",
    "\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682441212092,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "Pu0UfRuAQgnw",
    "outputId": "66b99b21-90d2-4e81-bc2b-d2686a818ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1\n",
      " 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "pred=rf_clf.predict(n)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1682441029213,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "wmAyfQzJCx40"
   },
   "outputs": [],
   "source": [
    "j=[[1,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [1,0,0,1,0],\n",
    "   [1,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,1],\n",
    "   [1,0,0,1,0],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [1.0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,1,1],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [1,0,0,1,0],\n",
    "   [1,0,0,1,0],\n",
    "   [1,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [1,0,0,0,0],\n",
    "   [1,0,0,0,0],\n",
    "   [1,0,0,0,0],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [1,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,1],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,1,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [0,0,0,0,0],\n",
    "   [1,0,0,1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "executionInfo": {
     "elapsed": 941,
     "status": "error",
     "timestamp": 1682441030141,
     "user": {
      "displayName": "Nevedan U",
      "userId": "02429391569345386637"
     },
     "user_tz": -330
    },
    "id": "wSwhaSQ4jEZ6",
    "outputId": "223e439d-6a1f-4f7a-d2e5-534744b7fb72"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d865b6b26bf0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#j = np.array(j).reshape(1, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrf_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;34m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    903\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[list([1, 0, 0, 0, 0]) list([0, 0, 0, 0, 0]) list([1, 0, 0, 0, 0])\n list([0, 0, 0, 1, 0]) list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 0, 0]) list([1, 0, 0, 1, 0])\n list([1, 0, 0, 1, 0]) list([0, 0, 0, 0, 0]) list([0, 0, 0, 0, 1])\n list([1, 0, 0, 1, 0]) list([1, 0, 0, 0, 0]) list([0, 0, 0, 0, 0])\n list([1.0, 0, 0, 0]) list([0, 0, 0, 1, 0]) list([1, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([1, 0, 0, 0, 0]) list([0, 0, 0, 1, 0])\n list([0, 0, 0, 1, 1]) list([1, 0, 0, 0, 0]) list([0, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0]) list([1, 0, 0, 1, 0])\n list([1, 0, 0, 1, 0]) list([1, 0, 0, 1, 0]) list([0, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0]) list([0, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0]) list([1, 0, 0, 0, 0])\n list([1, 0, 0, 0, 0]) list([1, 0, 0, 0, 0]) list([1, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0]) list([1, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 0, 1]) list([0, 0, 0, 1, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 1, 0]) list([0, 0, 0, 0, 0])\n list([0, 0, 0, 0, 0]) list([0, 0, 0, 0, 0]) list([1, 0, 0, 1, 0])].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "#j = np.array(j).reshape(1, -1)\n",
    "pred=nb_clf.predict(j)\n",
    "print(pred)\n",
    "pred=rf_clf.predict(j)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
